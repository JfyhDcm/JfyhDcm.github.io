<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HMM</title>
    <url>/2021/10/25/HMM/</url>
    <content><![CDATA[<h1 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h1><h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>简单来说，熵是表示物质系统状态的一种度量，用它老表征系统的无序程度。<em><strong>熵越大，系统越无序</strong></em>，意味着系统结构和运动的不确定和无规则；反之，<em><strong>熵越小，系统越有序</strong></em>，意味着具有确定和有规则的运动状态。熵的中文意思是热量被温度除的商。负熵是物质系统有序化，组织化，复杂化状态的一种度量。</p>
<p>熵最早来原于<strong>物理学</strong>. 德国物理学家鲁道夫·克劳修斯首次提出熵的概念，用来表示任何一种能量在空间中分布的均匀程度，能量分布得越均匀，熵就越大。</p>
<h2 id="隐马尔科夫模型"><a href="#隐马尔科夫模型" class="headerlink" title="隐马尔科夫模型"></a>隐马尔科夫模型</h2><p><strong>隐马尔可夫模型</strong>（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。</p>
<p>是在被建模的系统被认为是一个马尔可夫过程与未观测到的（隐藏的）的状态的统计马尔可夫模型。</p>
<p>下面用一个简单的例子来阐述：</p>
<p>假设我手里有三个不同的骰子。第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrji6v89zj60gn091aan02.jpg" alt="image" style="zoom:75%;" />

<p>假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。不停的重复上述过程，我们会得到一串数字，每个数字都是1，2，3，4，5，6，7，8中的一个。例如我们可能得到这么一串数字（掷骰子10次）：1 6 3 5 2 7 3 5 2 4</p>
<p>这串数字叫做可见状态链。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串隐含状态链。在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8</p>
<p>一般来说，HMM中说到的马尔可夫链其实是指隐含状态链，因为隐含状态（骰子）之间存在转换概率（transition probability）。在我们这个例子里，D6的下一个状态是D4，D6，D8的概率都是1/3。D4，D8的下一个状态是D4，D6，D8的转换概率也都一样是1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1。这样就是一个新的HMM。</p>
<p>同样的，尽管可见状态之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做输出概率（emission probability）。就我们的例子来说，六面骰（D6）产生1的输出概率是1/6。产生2，3，4，5，6的概率也都是1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjhxhdlzj60fz08njrx02.jpg" alt="image"></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjhrfzs3j609c08oaa302.jpg" alt="image"></p>
<p>其实对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。这些算法我会在下面详细讲。</p>
<p>×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××<br>   如果你只想看一个简单易懂的例子，就不需要往下看了。<br>   ×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××<br>  说两句废话，答主认为呢，要了解一个算法，要做到以下两点：会其意，知其形。答主回答的，其实主要是第一点。但是这一点呢，恰恰是最重要，而且很多书上不会讲的。正如你在追一个姑娘，姑娘对你说“你什么都没做错！”你要是只看姑娘的表达形式呢，认为自己什么都没做错，显然就理解错了。你要理会姑娘的意思，“你赶紧给我道歉！”这样当你看到对应的表达形式呢，赶紧认错，跪地求饶就对了。数学也是一样，你要是不理解意思，光看公式，往往一头雾水。不过呢，数学的表达顶多也就是晦涩了点，姑娘的表达呢，有的时候就完全和本意相反。所以答主一直认为理解姑娘比理解数学难多了。</p>
<p>回到正题，和HMM模型相关的算法主要分为三类，分别解决三种问题：<br> <em><strong>1）知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道每次掷出来的都是哪种骰子（隐含状态链）。</strong></em><br>   这个问题呢，在语音识别领域呢，叫做解码问题。这个问题其实有两种解法，会给出两个不同的答案。每个答案都对，只不过这些答案的意义不一样。第一种解法求最大似然状态路径，说通俗点呢，就是我求一串骰子序列，这串骰子序列产生观测结果的概率最大。第二种解法呢，就不是求一组骰子序列了，而是求每次掷出的骰子分别是某种骰子的概率。比如说我看到结果后，我可以求得第一次掷骰子是D4的概率是0.5，D6的概率是0.3，D8的概率是0.2.第一种解法我会在下面说到，但是第二种解法我就不写在这里了，如果大家有兴趣，我们另开一个问题继续写吧。</p>
<p><em><strong>2）还是知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道掷出这个结果的概率。</strong></em><br>   看似这个问题意义不大，因为你掷出来的结果很多时候都对应了一个比较大的概率。问这个问题的目的呢，其实是检测观察到的结果和已知的模型是否吻合。如果很多次结果都对应了比较小的概率，那么就说明我们已知的模型很有可能是错的，有人偷偷把我们的骰子給换了。</p>
<p><em><strong>3）知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果（可见状态链），我想反推出每种骰子是什么（转换概率）。</strong></em><br>   这个问题很重要，因为这是最常见的情况。很多时候我们只有可见结果，不知道HMM模型里的参数，我们需要从可见结果估计出这些参数，这是建模的一个必要步骤。</p>
<p>问题阐述完了，下面就开始说解法。（0号问题在上面没有提，只是作为解决上述问题的一个辅助）<br>   0.一个简单问题<br>其实这个问题实用价值不高。由于对下面较难的问题有帮助，所以先在这里提一下。<br>知道骰子有几种，每种骰子是什么，每次掷的都是什么骰子，根据掷骰子掷出的结果，求产生这个结果的概率。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjhomoeaj60a006mwen02.jpg" alt="image"></p>
<p>解法无非就是概率相乘：<br><img src="http://zhihu.com/equation?tex=P=P(D6)*P(D6%5Crightarrow+1)*P(D6%5Crightarrow+D8)*P(D8%5Crightarrow+6)*P(D8%5Crightarrow+D8)*P(D8%5Crightarrow+3)" alt="img"><br><img src="http://zhihu.com/equation?tex==%5Cfrac%7B1%7D%7B3%7D+*%5Cfrac%7B1%7D%7B6%7D+*%5Cfrac%7B1%7D%7B3%7D+*%5Cfrac%7B1%7D%7B8%7D+*%5Cfrac%7B1%7D%7B3%7D+*%5Cfrac%7B1%7D%7B8%7D+" alt="img"></p>
<p><strong>1.看见不可见的，破解骰子序列</strong><br>   这里我说的是第一种解法，解最大似然路径问题。<br>   举例来说，我知道我有三个骰子，六面骰，四面骰，八面骰。我也知道我掷了十次的结果（1 6 3 5 2 7 3 5 2 4），我不知道每次用了那种骰子，我想知道最有可能的骰子序列。</p>
<p>其实最简单而暴力的方法就是穷举所有可能的骰子序列，然后依照第零个问题的解法把每个序列对应的概率算出来。然后我们从里面把对应最大概率的序列挑出来就行了。如果马尔可夫链不长，当然可行。如果长的话，穷举的数量太大，就很难完成了。<br>   另外一种很有名的算法叫做Viterbi algorithm. 要理解这个算法，我们先看几个简单的列子。<br>   首先，如果我们只掷一次骰子：<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjhdujbxj601c035mwx02.jpg" alt="image"></p>
<p>看到结果为1.对应的最大概率骰子序列就是D4，因为D4产生1的概率是1/4，高于1/6和1/8.<br>   把这个情况拓展，我们掷两次骰子：<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjhb2n0kj602z02xa9v02.jpg" alt="image"></p>
<p>结果为1，6.这时问题变得复杂起来，我们要计算三个值，分别是第二个骰子是D6，D4，D8的最大概率。显然，要取到最大概率，第一个骰子必须为D4。这时，第二个骰子取到D6的最大概率是<br><img src="http://zhihu.com/equation?tex=P2(D6)=P(D4)*P(D4%5Crightarrow+1)*P(D4%5Crightarrow+D6)*P(D6%5Crightarrow+6)" alt="img"><br><img src="http://zhihu.com/equation?tex==%5Cfrac%7B1%7D%7B3%7D+*%5Cfrac%7B1%7D%7B4%7D+*%5Cfrac%7B1%7D%7B3%7D+*%5Cfrac%7B1%7D%7B6%7D" alt="img"><br>   同样的，我们可以计算第二个骰子是D4或D8时的最大概率。我们发现，第二个骰子取到D6的概率最大。而使这个概率最大时，第一个骰子为D4。所以最大概率骰子序列就是D4 D6。<br>继续拓展，我们掷三次骰子：<img src="https://images0.cnblogs.com/blog/133059/201507/161450394547765.png" alt="image"></p>
<p>同样，我们计算第三个骰子分别是D6，D4，D8的最大概率。我们再次发现，要取到最大概率，第二个骰子必须为D6。这时，第三个骰子取到D4的最大概率是<img src="http://zhihu.com/equation?tex=P3(D4)=P2(D6)*P(D6%5Crightarrow+D4)*P(D4%5Crightarrow+3)" alt="img"><br><img src="http://zhihu.com/equation?tex==%5Cfrac%7B1%7D%7B216%7D+*%5Cfrac%7B1%7D%7B3%7D+*%5Cfrac%7B1%7D%7B4%7D" alt="img"><br>   同上，我们可以计算第三个骰子是D6或D8时的最大概率。我们发现，第三个骰子取到D4的概率最大。而使这个概率最大时，第二个骰子为D6，第一个骰子为D4。所以最大概率骰子序列就是D4 D6 D4。</p>
<p>写到这里，大家应该看出点规律了。既然掷骰子一二三次可以算，掷多少次都可以以此类推。我们发现，我们要求最大概率骰子序列时要做这么几件事情。首先，不管序列多长，要从序列长度为1算起，算序列长度为1时取到每个骰子的最大概率。然后，逐渐增加长度，每增加一次长度，重新算一遍在这个长度下最后一个位置取到每个骰子的最大概率。因为上一个长度下的取到每个骰子的最大概率都算过了，重新计算的话其实不难。当我们算到最后一位时，就知道最后一位是哪个骰子的概率最大了。然后，我们要把对应这个最大概率的序列从后往前推出来。<br><strong>2.谁动了我的骰子？</strong><br>   比如说你怀疑自己的六面骰被赌场动过手脚了，有可能被换成另一种六面骰，这种六面骰掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。你怎么办么？答案很简单，算一算正常的三个骰子掷出一段序列的概率，再算一算不正常的六面骰和另外两个正常骰子掷出这段序列的概率。如果前者比后者小，你就要小心了。<br>   比如说掷骰子的结果是：<br><a href="http://images0.cnblogs.com/blog/133059/201507/161450402987108.png"><img src="https://images0.cnblogs.com/blog/133059/201507/161450406884821.png" alt="image"></a></p>
<p>要算用正常的三个骰子掷出这个结果的概率，其实就是将所有可能情况的概率进行加和计算。同样，简单而暴力的方法就是把穷举所有的骰子序列，还是计算每个骰子序列对应的概率，但是这回，我们不挑最大值了，而是把所有算出来的概率相加，得到的总概率就是我们要求的结果。这个方法依然不能应用于太长的骰子序列（马尔可夫链）。<br>   我们会应用一个和前一个问题类似的解法，只不过前一个问题关心的是概率最大值，这个问题关心的是概率之和。解决这个问题的算法叫做前向算法（forward algorithm）。<br>首先，如果我们只掷一次骰子：</p>
<p><a href="http://images0.cnblogs.com/blog/133059/201507/161450412515950.png"><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjihyudaj601n033q2p02.jpg" alt="image"></a></p>
<p>看到结果为1.产生这个结果的总概率可以按照如下计算，总概率为0.18：</p>
<p><a href="http://images0.cnblogs.com/blog/133059/201507/161450427046706.png"><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjig6i68j60gh04rglo02.jpg" alt="image"></a></p>
<p>把这个情况拓展，我们掷两次骰子：</p>
<p><a href="http://images0.cnblogs.com/blog/133059/201507/161450442206677.png"><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjieaxkvj602y02w74302.jpg" alt="image"></a></p>
<p>看到结果为1，6.产生这个结果的总概率可以按照如下计算，总概率为0.05：</p>
<p><a href="http://images0.cnblogs.com/blog/133059/201507/161450450328560.png"><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjic0ladj60gk04t3yq02.jpg" alt="image"></a></p>
<p>继续拓展，我们掷三次骰子：</p>
<p><a href="http://images0.cnblogs.com/blog/133059/201507/161450469549473.png"><img src="https://images0.cnblogs.com/blog/133059/201507/161450480322045.png" alt="image"></a></p>
<p>看到结果为1，6，3.产生这个结果的总概率可以按照如下计算，总概率为0.03：</p>
<p><a href="http://images0.cnblogs.com/blog/133059/201507/161450499388728.png"><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjila0mqj60gh04uq3d02.jpg" alt="image"></a></p>
<p>同样的，我们一步一步的算，有多长算多长，再长的马尔可夫链总能算出来的。用同样的方法，也可以算出不正常的六面骰和另外两个正常骰子掷出这段序列的概率，然后我们比较一下这两个概率大小，就能知道你的骰子是不是被人换了。</p>
<p>原文：<a href="https://www.cnblogs.com/skyme/p/4651331.html">https://www.cnblogs.com/skyme/p/4651331.html</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title>词嵌入1:BagofWords</title>
    <url>/2021/10/24/WordEmbedding1:BagofWords/</url>
    <content><![CDATA[<ul>
<li><em><strong>基于one-hot、tf-idf、textrank等的bag-of-words；</strong></em></li>
<li><strong>主题模型：LSA（SVD）、pLSA、LDA；</strong></li>
<li><strong>基于词向量的固定表征：word2vec、fastText、glove</strong></li>
<li><strong>基于词向量的动态表征：elmo、GPT、bert</strong></li>
</ul>
<h1 id="Bag-of-words"><a href="#Bag-of-words" class="headerlink" title="Bag-of-words"></a>Bag-of-words</h1><h2 id="TF-IFD算法"><a href="#TF-IFD算法" class="headerlink" title="TF-IFD算法"></a>TF-IFD算法</h2><p>TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用加权技术。</p>
<p>   TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在<em><strong>文件</strong></em>中出现的次数成正比增加，但同时会随着它在<em><strong>语料库</strong></em>中出现的频率成反比下降。</p>
<p>   TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p>
<p>TF词频 公式：<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvq88r14w2j605l0203yc02.jpg" alt="img"></p>
<p>即：<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvq89qvhplj609m01wt8p02.jpg" alt="img"></p>
<p>IDF 逆向文件频率：<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvq8a9n46bj607w02a0sm02.jpg" alt="img"></p>
<p>即：<img src="https://img-blog.csdn.net/20180807191126207?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>TF-IDF为：<img src="https://img-blog.csdn.net/201808071912424?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<ul>
<li>优点：1. 可用某篇文章中出现次数多但在其他文章中出现次数少的词来作为该篇文章的特征词。2. 使罕见的单词更加突出并且有效地忽略了常用单词 3. 易于理解</li>
<li>缺点：1. 因为是词袋模型，所以没有考虑词的位置信息，但词的位置是有一定含义的。2. 并不能反映单词的重要程度和特征词的分布情况。</li>
</ul>
<h2 id="TextRank"><a href="#TextRank" class="headerlink" title="TextRank"></a>TextRank</h2><h3 id="PageRank算法"><a href="#PageRank算法" class="headerlink" title="PageRank算法"></a>PageRank算法</h3><ul>
<li>如果一个网页被很多其他网页链接到的话说明这个网页比较重要，也就是PageRank值会相对较高</li>
<li>如果一个PageRank值很高的网页链接到一个其他的网页，那么被链接到的网页的PageRank值会相应地因此而提高</li>
</ul>
<img src="https://raw.githubusercontent.com/evilKing/storage_image/master/page_rank1.png" alt="img" style="zoom:50%;" />

<p>PageRank算法预先给每个网页一个 PR值（下面用 PR值指代 PageRank值），由于 PR值物理意义上为一个网页被访问概率，所以一般是1/N ，其中 N 为网页总数。另外，一般情况下，所有网页的 PR值的总和为 1。如果不为 1的话也不是不行，最后算出来的不同网页之间 PR值的大小关系仍然是正确的，只是不能直接地反映概率了。</p>
<p><img src="https://raw.githubusercontent.com/evilKing/storage_image/master/page_rank.jpg" alt="img"></p>
<p>互联网中的众多网页可以看作一个有向图。预先给定 PR值后，通过下面的算法不断迭代，直至达到平稳分布为止。这时 A 的 PR值就可以表示为 :<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvqf8d903cj60de02ewee02.jpg" alt="image-20211024111841444" style="zoom:50%;" />然而图中除了 C 之外，B 和 D 都不止有一条出链，所以上面的计算式并不准确。想象一个用户现在浏览 B 网页，那么下一步他打开 A 网页还是 D网页在统计上应该是相同概率的。所以 A 的 PR值应该表述为 :<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvqf8can5wj60fe03qt8n02.jpg" alt="image-20211024112044985" style="zoom:50%;" /></p>
<p>互联网中不乏一些没有出链的网页，如图:</p>
<p><img src="https://raw.githubusercontent.com/evilKing/storage_image/master/page_rank3.jpg" alt="img"></p>
<p>图中的 C 网页没有出链，对其他网页没有 PR值的贡献，我们不喜欢这种自私的网页（其实是为了满足 Markov 链的收敛性），于是设定其对所有的网页（包括它自己）都有出链，则此图中A的PR值（设定C也连到了A）可表示为：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvqf8g2hmmj60e604gwef02.jpg" alt="image-20211024112321973" style="zoom:50%;" />

<p>然而我们再考虑一种情况：互联网中一个网页只有对自己的出链，或者几个网页的出链形成一个循环圈。那么在不断地迭代过程中，这一个或几个网页的PR值将只增不减，显然不合理。如下图中的 C 网页就是刚刚说的只有对自己的出链的网页：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvqf8e4r2zj60hu0fc3yx02.jpg" alt="image-20211024112609155" style="zoom:50%;" />

<p>为了解决这个问题，我们想象一个随机浏览网页的人，当他到达 C 网页后，显然不会傻傻地一直被 C 网页的小把戏困住。我们假定他有一个确定的概率会输入网址直接跳转到一个随机的网页，并且跳转到每个网页的概率是一样的。于是则此图中 A 的 PR值可表示为：<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvqf8f4rwvj60hy04wjrc02.jpg" alt="image-20211024112634051" style="zoom:50%;" />这里的α是阻尼系数，google系统选用0.85，表示用户愿意按照网页跳转指示浏览网页的概率。</p>
<p>总结一下：<img src="https://pic1.zhimg.com/80/v2-0d6c3c27eeee373e89b02e57630df6e4_1440w.jpg" alt="img"></p>
<h3 id="TextRank-1"><a href="#TextRank-1" class="headerlink" title="TextRank"></a>TextRank</h3><p>网页之间的链接关系可以用图表示，那么怎么把一个句子（可以看作词的序列）构建成图呢？TextRank将某一个词与其前面的N个词、以及后面的N个词均具有图相邻关系（类似于N-gram语法模型）。</p>
<p>具体实现：设置一个长度为N的滑动窗口，所有在这个窗口之内的词都视作词结点的相邻结点；则TextRank构建的词图为无向图。下图给出了由一个文档构建的词图（去掉了停用词并按词性做了筛选）：</p>
<img src="https://pic2.zhimg.com/80/v2-45e33818add08141d4deea618d1bd6c9_1440w.jpg" alt="img" style="zoom: 50%;" />

<p>考虑到不同词对可能有不同的共现（co-occurrence），TextRank将共现作为无向图边的权值。那么，TextRank的迭代计算公式如下：<img src="https://pic2.zhimg.com/80/v2-04b2c330a7f471e8b93f78963e6c1619_1440w.jpg" alt="img"></p>
<p>d为阻尼系数；$w_{ij}$为权重，用来表示两个节点间边连接的不同重要程度。</p>
<p><em>最后求得各节点权重。</em></p>
<h3 id="TextRank用户关键词提取"><a href="#TextRank用户关键词提取" class="headerlink" title="TextRank用户关键词提取"></a>TextRank用户关键词提取</h3><p>1)把给定的文本T按照完整句子进行分割，即</p>
<p><img src="https://pic2.zhimg.com/80/v2-8c209ef93e503864960a934bc6a5b389_1440w.jpg" alt="img"></p>
<p>2)对于每个句子Si属于T，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即</p>
<p><img src="https://pic3.zhimg.com/80/v2-d9ab45963e0e227ebc98e871802fa31a_1440w.jpg" alt="img"></p>
<p>，其中 ti,j 是保留后的候选关键词。</p>
<p>　　3)构建候选关键词图G = (V,E)，其中V为节点集，由（2）生成的候选关键词组成，然后采用共现关系（co-occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，K表示窗口大小，即最多共现K个单词。</p>
<p>　　4)根据上面公式，迭代传播<em>各节点的权重</em>，直至收敛。</p>
<p>　　5)对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词。</p>
<p>　　6)由5得到最重要的T个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。</p>
<p><strong>补充：提取关键词短语</strong>：提取关键词短语的方法基于关键词提取，可以简单认为：如果提取出的若干关键词在文本中相邻，那么构成一个被提取的关键短语。</p>
<p><strong>停用词：</strong>是指在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为Stop Words（停用词）。这些停用词都是人工输入、非自动化生成的，生成后的停用词会形成一个停用词表。但是，并没有一个明确的停用词表能够适用于所有的工具。甚至有一些工具是明确地避免使用停用词来支持短语搜索的。</p>
<h3 id="TextRank生成文章摘要"><a href="#TextRank生成文章摘要" class="headerlink" title="TextRank生成文章摘要"></a>TextRank生成文章摘要</h3><p>　　将文本中的每个句子分别看做一个节点，如果两个句子有相似性，那么认为这两个句子对应的节点之间存在一条无向有权边。考察句子相似度的方法是下面这个公式：</p>
<p><img src="https://img-blog.csdnimg.cn/20181104103726372.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpYW45OQ==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>公式中，Si,Sj分别表示两个句子词的个数总数，Wk表示句子中的词，那么分子部分的意思是同时出现在两个句子中的同一个词的个数，<u>分母是对句子中词的个数求对数之和。分母这样设计可以遏制较长的句子在相似度计算上的优势。</u></p>
<p>我们可以根据以上相似度公式循环计算任意两个节点之间的相似度，根据阈值去掉两个节点之间相似度较低的边连接，构建出节点连接图，然后计算TextRank值，最后对所有TextRank值排序，选出TextRank值最高的几个节点对应的句子作为摘要。</p>
<h2 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h2><p>​    Bag of words模型，也叫做“词袋”，在信息检索中，Bag of words model假定对于一个文本，忽略其词序和语法、句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文本中每个词的出现都是独立的，不依赖于其他词是否出现，或者说当这篇文章的作者在任意一个位置选择一个词汇都不受前面句子的影响而独立选择的。</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>词向量</tag>
        <tag>TF-IDF</tag>
        <tag>TextRank</tag>
        <tag>PageRank</tag>
      </tags>
  </entry>
  <entry>
    <title>词嵌入2：主题模型</title>
    <url>/2021/10/24/WordEmbedding2:ThemeModel/</url>
    <content><![CDATA[<ul>
<li><strong>基于one-hot、tf-idf、textrank等的bag-of-words；</strong></li>
<li><em><strong>主题模型：LSA（SVD）、pLSA、LDA；</strong></em></li>
<li><strong>基于词向量的固定表征：word2vec、fastText、glove</strong></li>
<li><strong>基于词向量的动态表征：elmo、GPT、bert</strong></li>
</ul>
<h1 id="主题模型"><a href="#主题模型" class="headerlink" title="主题模型"></a>主题模型</h1><p>各种模型的优缺点：</p>
<ul>
<li>One-hot：维度灾难 and 语义鸿沟</li>
<li>矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大</li>
<li>基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题</li>
<li>word2vec、fastText：优化效率高，但是基于局部语料</li>
<li>glove：基于全局预料，结合了LSA和word2vec的优点</li>
<li>elmo、GPT、bert：动态特征</li>
</ul>
<h2 id="LSA（隐语义分析）"><a href="#LSA（隐语义分析）" class="headerlink" title="LSA（隐语义分析）"></a>LSA（隐语义分析）</h2><h3 id="SVD（奇异值分解）"><a href="#SVD（奇异值分解）" class="headerlink" title="SVD（奇异值分解）"></a>SVD（奇异值分解）</h3><p>LSA(latent semantic analysis)潜层语义分析，也被称为LSI(latent semantic index)，是Scott Deerwester, Susan T. Dumais等人在1990年提出来的一种新的索引和检索算法。该算法和传统向量空间模型(vector space model)一样使用向量来表示词(word)和文档(documents)，vsm是通过向量间的关系(如夹角)来判断词及文档间的关系，而不同的 是，LSA将词和文档映射到潜在语义空间，从而去除了原始向量空间中的一些“噪音”，提高了信息检索的精确度。我们下面就先从推荐系统的角度简单的解释一下，什么是浅语意空间。</p>
<p>这里简单的根据推荐系统来说，这里使用给用户推荐电影的例子来解释，首先我们会得到一个评分矩阵，这个矩阵呢它的行是item（主题），如果推荐的是电影的话，那么item就是电影名了，而列就是用户了即user，我们从不同的用户中就可以得到这样的评分矩阵，如下所示：</p>
<p><img src="https://img-blog.csdnimg.cn/20181219094042449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5ODY1OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>现在的问题是，1、这个评分矩阵很大。2、这个矩阵是稀疏，什么意思呢？假如这个矩阵是给电影评分的，那么我们不能得到所有用户给所有的电影评分，因此这个矩阵中会有很多的空缺值。因此推荐系统中会有这两个问题的存在，如何解决呢？这里使用的是SVD 进行解决，我们看看他是如何解决推荐系统的两个问题的，其实很简单，如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/2018121910050073.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5ODY1OA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>假如我们的评分矩阵是nxm的矩阵，其中m代表user，n代表item，我们可以通过两个矩阵的相乘的形式得到评分矩阵，写成上图等号右边的形式。</p>
<p>上图我们可看到评分矩阵可以通过nxk的矩阵和kxm的矩阵相乘得到，这里的k是隐分类的意思，<u>这里的隐分类不是我们人为划分的，一般都是我们通过矩阵分解以后得到这个k，</u>然后通过两个矩阵的内容进行判断隐分类的含义。</p>
<p>这里以电影评分矩阵进行比喻，一旦通过矩阵分解成上面的形式，我就可以得到电影评分的矩阵分解了，就会得到隐分类了，此时我们观察分解后的矩阵就会发现，隐分类代表的是某一类型电影，如上图等号右边的第一个矩阵即nxk矩阵，就是说对这n个电影给出概率或者打分来说明他是属于电影的哪一类如科幻、爱情、恐怖等等，此时的k就是代表电影的类型了也就是隐分类了，根据每部电影的行中不同的值来判断它属于那个电影类型的概率，这就是上图等号右边第一个矩阵的含义，那么等号右边的后面的那个矩阵代表什么呢？其实很类似的分析，每个用户代表着一列，那么每行代表在不同类型的电影，我们通过分析用户观看哪种类型的电影概率或者评分最高，以此来给用户推荐他喜欢的类型电影。</p>
<p>由此我们可以发现在电影的评分中隐分类就代表着电影的类型，而分解后的两个矩阵对我们都很有用，这就是我们k的物理意义和分解后矩阵代表的含义，但是我们还没解释他是如何解决推荐系统的两个问题的，下面我们就来解释一下：假如我们分解后的矩阵的k值通常要比n和m小的多数值，如n=100万部电影，k=100种类型，m=100万人，我们来计算一下分解后的数据有多少个：nxk+kxm = 100万x100 + 100x100万 = 2亿个数据，需要2亿个存储单元，我们看看原始评分矩阵的大小：nxm= 100万x100万=10000亿，可想而知，存储量很大，因此矩阵分解可以起到降维、压缩数据的作用。因为数据压缩了其实就解决了稀疏性的问题，稍后大家会看到如何解决的，上面就是推荐系统的应用，那么在文本分类是如何使用的呢？</p>
<h3 id="LSA用于文本分类"><a href="#LSA用于文本分类" class="headerlink" title="LSA用于文本分类"></a>LSA用于文本分类</h3><img src="https://img-blog.csdnimg.cn/20181219105441296.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5ODY1OA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

<p>每篇文章我们都可以构造一个词向量，这样由多篇文章就可以构成一个矩阵，如上图的等号左边的图，这里的m就是文章数了，n就是每个词袋的元素数了，通过矩阵分解后就会得到像上图等号右边的第一个矩阵形式，和推荐系统类似，这里的k代表的就是隐分类，他的物理意义就是说在每个词向量（文本）的元素中概率或者评分相近的词，就是同义词和近义词了，那么第二个矩阵就相当于降维了，即我们关键的信息留下来，同义词和近义词我们按照左边的分解取一个概率最大的表示，如我们讨论两个文本是否相似时，我们知道评分矩阵的维度是很大，而且还是稀疏的，但是我们通过矩阵分解以后就会得到隐特征，即主要的信息都保存下来了，此时我们的矩阵就不在是稀疏了，这也就解释了为什么起到降维和解决稀疏矩阵的原理了。</p>
<p>这里大家不应太纠结隐分类到底代表什么，我们不需要知道，只需要判断即可，这就是SVD用在文本分类中的原理，因为文本分类是最早使用SVD的，因此，推荐系统是借鉴了文本分类的应用才研究出推荐系统的，这里就不在过多讨论了，大家肯定想知道，这个矩阵分解到底是如何做呢？怎么得到这个k的值呢？如何确定呢？是人为确定还是自动生成呢？我们的主要精力就来看看SVD到底是如何分解的。答：奇异值矩阵分解。</p>
<h2 id="PLSA（基于概率的隐语意分析）"><a href="#PLSA（基于概率的隐语意分析）" class="headerlink" title="PLSA（基于概率的隐语意分析）"></a>PLSA（基于概率的隐语意分析）</h2><img src="https://img-blog.csdnimg.cn/20181219105441296.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5ODY1OA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:80%;" />



<p>我们通过矩阵的分解对文本数据进行压缩，压缩量很可观，尤其是原始的矩阵的维度很高时压缩的更可观，因为k通常要远远小于n。</p>
<p>如上图等号左边的矩阵其实就是我们的文本的词向量组成的，我们知道一篇文章的词是很多的，而且还是稀疏的，如果一旦文章数也很多,那么整个矩阵的元素会很大很大，但是通过矩阵分解就会减少很多。上图中的每一列都代表一个文本的词向量，里面的值是词向量的权值，那么我们分解后的矩阵分别代表什么呢？其中k就是隐分类或者说是隐特征，如果多个词在同一列出现就说明这几个词是相近的，在最右边的矩阵是什么意思呢？可以理解为经过压缩后的文本特征，每个文本都有k个特征，然后对比一下，以此来判断是否相似。</p>
<p>还有一种用法就是我先通过矩阵分解，然后在组合成nxm的矩阵，这个新组合的矩阵和原始矩阵很像，但是肯定不一样，因为我们中间在特征值时略去了部分特征值，怎么理解这一步呢？大家可以理解为我们是<em>去除噪声</em>，留下的才是主要特征，和PCA技术差不多。但是这个方法就没有缺点吗？有的，下面我们来看看：</p>
<p>缺点是SVD的分解的计算量太大，计算复杂度高，当矩阵达到1000维以上时计算已经非常缓慢，但文本分析一般都会形成非常大型的“文档-词”矩阵，从而难以实现，甚至存储都很困难。因此需要引入新的求解SVD的方法，这时一种新型的求解思路就出来了，下面还以推荐系统的电影分类为例进行讲解：</p>
<img src="https://img-blog.csdnimg.cn/2018121910050073.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5ODY1OA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

<p>按照我们上一节的内容就是当得到评分矩阵时，就可以直接对其进行分解，但是问题是当这个评分矩阵的维度很大时，SVD的计算量将很大，为了避免这么大的计算量，我们采用这样的做法，就是我把上图的等号右边的两个矩阵的元素全看做 变量，对，你没看错，全看做变量，那么我让他们相乘就会得到和原始评分矩阵维度一样的矩阵，那么这个时候我把原始矩阵已知的值和计算出来的矩阵相对应位置的值尽量相等，这样就建立了优化函数，然后使用优化算法使其误差达到最低，一旦我们合成的矩阵符合我们的要求，同时就可推测原始评分稀疏矩阵的值了，通过预测的值的大小就可以推荐给用户分值高的电影了，下面结合图来讲：</p>
<img src="https://img-blog.csdnimg.cn/2018121919005668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjM5ODY1OA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 67%;" />

<p>首先我们构建两个矩阵维度为nxk和kxm的，如上图的中间部分，那么我把这两个矩阵的元素都看做变量，这时候把矩阵相乘得到合成评分矩阵B，如上图的右端，那么把合成的数据和原始的评分矩阵A对应的位置相比较如上图的A蓝色点，黄色点是缺少的（意思是用户没有看过该影片），使他们尽量的接近，怎么衡量呢？误差平方和函数呀，因此可以得到关于很多变量的误差函数L，此时我们使用优化算法进行优化，一旦达到我们的要求，那么说明我们的分解矩阵就确定了，此时使用这个 再去预测A矩阵的位置的黄色的点，这可以对评分矩阵的稀缺值进行填充，进而根据分值大小给用户推荐电影了，就是这个原理。这样就巧妙的避免了求解SVD了，一旦得到优化函数，我们就可以通过梯度下降得到我们的极值。</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>词向量</tag>
        <tag>LSA</tag>
        <tag>SVD</tag>
        <tag>pLSA</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/06/01/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>CRF</title>
    <url>/2021/11/11/CRF/</url>
    <content><![CDATA[<h1 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h1>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>从极大似然到EM</title>
    <url>/2021/11/12/%E4%BB%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%88%B0EM/</url>
    <content><![CDATA[<h1 id="从极大似然到EM"><a href="#从极大似然到EM" class="headerlink" title="从极大似然到EM"></a>从极大似然到EM</h1><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p><a href="https://zhuanlan.zhihu.com/p/26614750">https://zhuanlan.zhihu.com/p/26614750</a></p>
<h2 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h2><p><a href="https://zhuanlan.zhihu.com/p/40991784">https://zhuanlan.zhihu.com/p/40991784</a></p>
<p>理解：有一个隐式变量z和显式变量，让它们互相配合，达到和现在结果最一致的最大似然</p>
<p>E-step:求后验概率 M-step:最大化似然概率</p>
<h2 id="先验概率和后验概率"><a href="#先验概率和后验概率" class="headerlink" title="先验概率和后验概率"></a>先验概率和后验概率</h2><p><a href="https://zhuanlan.zhihu.com/p/26464206">https://zhuanlan.zhihu.com/p/26464206</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>极大似然</tag>
        <tag>数学</tag>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/2021/06/15/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<h3 id="数塔问题"><a href="#数塔问题" class="headerlink" title="数塔问题"></a><center>数塔问题</center></h3><p>当前的位置一定要走，从左OR从右</p>
<h3 id="最长不下降子序列LIS"><a href="#最长不下降子序列LIS" class="headerlink" title="最长不下降子序列LIS"></a><center>最长不下降子序列LIS</center></h3><p>当前这个数一定要选，接着第一个OR第二个…当前的前一个数构成</p>
<h3 id="最长公共子序列LCS"><a href="#最长公共子序列LCS" class="headerlink" title="最长公共子序列LCS"></a><center>最长公共子序列LCS</center></h3><p>截止到当前这两个，如果相同则+1OR不同，去找前面最大的状态继承。也就是dp数组含义为截止到目前两个位置的最长公共子序列长度</p>
<h3 id="最大子序和"><a href="#最大子序和" class="headerlink" title="最大子序和"></a><center>最大子序和</center></h3><p>输入：nums = [-2,1,-3,4,-1,2,1,-5,4] </p>
<p>输出：6 </p>
<p>解释：连续子数组 [4,-1,2,1] 的和最大，为 6 。</p>
<p>思路：dp[i]代表以i结束的最大子序列和，对于每个当前的i，都是必选的。要么选择前面的加和，要么不选。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="keyword">int</span>* nums, <span class="keyword">int</span> numsSize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> dp[<span class="number">30000</span>];</span><br><span class="line">    dp[<span class="number">0</span>]=nums[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> max=nums[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;numsSize;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i]=(dp[i<span class="number">-1</span>]+nums[i]&gt;nums[i]?(dp[i<span class="number">-1</span>]+nums[i]):nums[i]);</span><br><span class="line">        max=(dp[i]&gt;max?dp[i]:max);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> max;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="最长回文子串"><a href="#最长回文子串" class="headerlink" title="最长回文子串"></a><center>最长回文子串</center></h3><p>输入：s = “babad” </p>
<p>输出：”bab” </p>
<p>解释：”aba” 同样是符合题意的答案。</p>
<p>思路：搜索边界是长度为1的字符串，搜索方式是按照字符串长度的递增进行搜索。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> * <span class="title">longestPalindrome</span><span class="params">(<span class="keyword">char</span> * s)</span></span>&#123;</span><br><span class="line"><span class="comment">//分析：动态规划中的边界，就是当只有一个字母时，肯定是回文。</span></span><br><span class="line"><span class="comment">//第二种边界就是有两个连续一样的字母，是回文</span></span><br><span class="line"><span class="comment">//针对这两点可以分开来，用if else 语句分别来判断</span></span><br><span class="line"><span class="keyword">int</span> length =(<span class="keyword">int</span>)<span class="built_in">strlen</span>(s);</span><br><span class="line"><span class="keyword">if</span>(length==<span class="number">0</span>||length==<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> s;</span><br><span class="line"></span><br><span class="line"><span class="keyword">bool</span> table[<span class="number">1000</span>][<span class="number">1000</span>];</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;length;i++)</span><br><span class="line">&#123;</span><br><span class="line"> table[i][i]=<span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> start=<span class="number">0</span>;<span class="keyword">int</span> len=<span class="number">1</span>;<span class="comment">//字符开始的位置和长度</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;length;j++)<span class="comment">//方向采用的是自上而下</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;j;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(s[i]==s[j])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(j-i&lt;<span class="number">3</span>)<span class="comment">//这个有一个，两个，三个，字母的时候，根本不用看里面，只要s[i]==s[j]就行</span></span><br><span class="line">            &#123;</span><br><span class="line">                table[i][j]=<span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">           <span class="keyword">else</span> </span><br><span class="line">               table[i][j]=table[i+<span class="number">1</span>][j<span class="number">-1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        table[i][j]=<span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(table[i][j]&amp;&amp;(j-i+<span class="number">1</span>)&gt;len)</span><br><span class="line">        &#123;</span><br><span class="line">            start = i;</span><br><span class="line">            len =j-i+<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">s[start+len]=<span class="string">&#x27;\0&#x27;</span>;</span><br><span class="line"><span class="keyword">return</span>(s+start);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="不同路径个数"><a href="#不同路径个数" class="headerlink" title="不同路径个数"></a><center>不同路径个数</center></h3><p>一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。  机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。  问总共有多少条不同的路?</p>
<p><img src="https://i.loli.net/2021/06/15/WX6FYAVml8seMCE.png" alt="robot_maze.png"></p>
<p>思路：边界，搜索方式。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">uniquePaths</span><span class="params">(<span class="keyword">int</span> m, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> dp[m][n];</span><br><span class="line">    <span class="built_in">memset</span>(dp, <span class="number">0</span>, <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">int</span>) * m * n);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        dp[i][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        dp[<span class="number">0</span>][i] = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; m; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt; n; j++) &#123;</span><br><span class="line">            dp[i][j] = dp[i - <span class="number">1</span>][j] + dp[i][j - <span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[m - <span class="number">1</span>][n - <span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="最小路径和"><a href="#最小路径和" class="headerlink" title="最小路径和"></a>最小路径和</h3><p>思路：第一行和第一列作为动归边界，按照左上到右下的顺序进行动归。</p>
<p><img src="https://i.loli.net/2021/06/15/PzLs1XGA9dqU5bS.jpg" alt="minpath.jpg"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">minPathSum</span><span class="params">(<span class="keyword">int</span>[][] grid)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> width = grid[<span class="number">0</span>].length, high = grid.length;</span><br><span class="line">        <span class="keyword">if</span> (high == <span class="number">0</span> || width == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 初始化</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; high; i++) grid[i][<span class="number">0</span>] += grid[i - <span class="number">1</span>][<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; width; i++) grid[<span class="number">0</span>][i] += grid[<span class="number">0</span>][i - <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; high; i++)</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt; width; j++)</span><br><span class="line">                grid[i][j] += Math.<span class="built_in">min</span>(grid[i - <span class="number">1</span>][j], grid[i][j - <span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">return</span> grid[high - <span class="number">1</span>][width - <span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="二叉搜索树的个数"><a href="#二叉搜索树的个数" class="headerlink" title="二叉搜索树的个数"></a>二叉搜索树的个数</h3><p>给定一个整数 <em>n</em>，求以 1 … <em>n</em> 为节点组成的二叉搜索树有多少种？</p>
<p>输入: 3 </p>
<p>输出: 5 </p>
<p>解释: 给定 n = 3, 一共有 5 种不同结构的二叉搜索树:    </p>
<p><img src="https://i.loli.net/2021/06/15/31jVGqlQrvu5aDp.png" alt="image.png">             </p>
<p>思路：每个大型二叉树的左右子树分别是两颗小型二叉树</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">numTrees</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> dp[<span class="number">1000</span>];   <span class="comment">//dp[i]是以1...i的二叉搜索树个数</span></span><br><span class="line">    dp[<span class="number">0</span>]=<span class="number">1</span>;</span><br><span class="line">    dp[<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line">    dp[<span class="number">2</span>]=<span class="number">2</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">3</span>;i&lt;=n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        dp[i]=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;=i;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            dp[i]+=dp[j<span class="number">-1</span>]*dp[i-j]; <span class="comment">//以某个数为顶，看左右为几个数组成的</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[n];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="子序列判断"><a href="#子序列判断" class="headerlink" title="子序列判断"></a>子序列判断</h3><p>给定字符串 s 和 t ，判断 s 是否为 t 的子序列。  字符串的一个子序列是原始字符串删除一些（也可以不删除）字符而不改变剩余字符相对位置形成的新字符串。（例如，”ace”是”abcde”的一个子序列，而”aec”不是）。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> n = s.<span class="built_in">length</span>();</span><br><span class="line"><span class="keyword">int</span> m = t.<span class="built_in">length</span>();</span><br><span class="line"><span class="keyword">if</span> (n &gt; m) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[m + <span class="number">1</span>][n + <span class="number">1</span>];<span class="comment">//dp[i][j]：表示字符串 t 的前 i 个元素和字符串 s 的前 j 个元素中公共子序列的长度。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=m;i++) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;=n;j++) &#123;</span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span> (s[i<span class="number">-1</span>] == <span class="built_in">t</span>(j - <span class="number">1</span>)) &#123;</span><br><span class="line"></span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>;</span><br><span class="line">                &#125; </span><br><span class="line">           <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">                    dp[i][j] = dp[i - <span class="number">1</span>][j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (dp[m][n] == n) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>







]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>朴素贝叶斯</title>
    <url>/2021/10/24/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><p>在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数𝑌=𝑓(𝑋)Y=f(X),要么是条件分布𝑃(𝑌|𝑋)P(Y|X)。但是朴素贝叶斯却是==生成方法==，也就是直接找出特征输出Y和特征X的联合分布𝑃(𝑋,𝑌)P(X,Y),然后用𝑃(𝑌|𝑋)=𝑃(𝑋,𝑌)/𝑃(𝑋)P(Y|X)=P(X,Y)/P(X)得出。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>贝叶斯公式是**==“执果索因”==**：发生了结果，那么哪个原因可能性大？</p>
<img src="https://pic2.zhimg.com/80/v2-5ea21d9dcaeb14a5a163324b271e05a9_1440w.jpg" alt="img" style="zoom: 50%;" />

<p>举个简单的例子：村子有且仅有两个小偷，小A和小B，根据统计A偷东西的可能性是0.2,B偷东西的可能性是0.8。如果A去偷，偷成功的概率是0.8， 如果B去偷，偷成功的概率是0.3。如果村子丢了一件东西，A和B谁是嫌疑犯的可能性更大？</p>
<p>H={丢东西}</p>
<p>A = {A去偷东西}</p>
<p>B = {B去偷东西}</p>
<p>P(A) + P(B) = 1</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvqhzcij90j611c0n4jut02.jpg" alt="image-20211024155658056" style="zoom:50%;" />



<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvqhzd1ys1j60pk040q3602.jpg" alt="image-20211024163655561" style="zoom:50%;" />，然后假设各个维度的x相互独立，这样方便计算。</p>
<p>现在发生了$x^{test}$，是那种$C^k$种类产生的概率比较大？先验+数据（似然），求后验概率，过程如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvqhzdu5coj615y0j60v002.jpg" alt="image-20211024163838725"></p>
<p>为了计算还会假设离散的样本符合伯努利分布，连续值符合正态分布。</p>
<p>原文：<a href="https://www.cnblogs.com/pinard/p/6069267.html">https://www.cnblogs.com/pinard/p/6069267.html</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2021/11/20/%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E5%B8%83%E5%92%8CBeta%E5%88%86%E5%B8%83/</url>
    <content><![CDATA[<p>Beta分布：<a href="https://blog.csdn.net/a358463121/article/details/52562940">https://blog.csdn.net/a358463121/article/details/52562940</a></p>
<p>详细讲解：<a href="https://blog.csdn.net/guleileo/article/details/80971601">https://blog.csdn.net/guleileo/article/details/80971601</a></p>
<p>变分推断ELBO：<a href="https://blog.csdn.net/qy20115549/article/details/93074519">https://blog.csdn.net/qy20115549/article/details/93074519</a></p>
<p>变分推断概率图：<a href="https://qianyang-hfut.blog.csdn.net/article/details/86644192">https://qianyang-hfut.blog.csdn.net/article/details/86644192</a></p>
]]></content>
  </entry>
  <entry>
    <title>背包问题</title>
    <url>/2021/06/01/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="背包问题"><a href="#背包问题" class="headerlink" title="背包问题"></a>背包问题</h2><p>背包问题属于动态规划，是一种对于每个元素存在两种状态（拿与不拿）的特殊动态规划问题。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li><p>逆向枚举：逆向是保证当前物品只被装入背包一次</p>
<p>正向枚举：正向枚举则当前物品多次装入背包</p>
</li>
<li><p>初始状态的设计</p>
<p>* </p>
</li>
</ul>
<h2 id="01背包"><a href="#01背包" class="headerlink" title="01背包"></a>01背包</h2><p>最基本的背包问题就是01背包问题（01 knapsack problem）：一共有N件物品，第i（i从1开始）件物品的重量为w[i]，价值为v[i]。在总重量不超过背包承载上限W的情况下，能够装入背包的最大价值是多少？</p>
<p>如果采用暴力穷举的方式，每件物品都存在装入和不装入两种情况，所以总的时间复杂度是O(2^N)，这是不可接受的。而使用动态规划可以将复杂度降至O(NW)。我们的目标是书包内物品的总价值，而变量是物品和书包的限重，所以我们可定义状态dp:</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i][j]表示将前i件物品装进限重为j的背包可以获得的最大价值, 0&lt;=i&lt;=N, 0&lt;=j&lt;=W</span><br></pre></td></tr></table></figure>
</blockquote>
<p>那么我们可以将d[0][0…W]初始化为0，表示将前0个物品（即没有物品）装入书包的最大价值为0。那么当 i &gt; 0 时<code>dp[i][j]</code>有两种情况：</p>
<ol>
<li>不装入第i件物品，即<code>dp[i−1][j]</code>；</li>
<li>装入第i件物品（前提是能装下），即<code>dp[i−1][j−w[i]] + v[i]</code>。</li>
</ol>
<p>即状态转移方程为</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dp[i][j] = max(dp[i−1][j], dp[i−1][j−w[i]]+v[i]) // j &gt;= w[i]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>由上述状态转移方程可知，<code>dp[i][j]</code>的值只与<code>dp[i-1][0,...,j-1]</code>有关，所以我们可以采用动态规划常用的方法（滚动数组）对空间进行优化（即去掉dp的第一维）。需要注意的是，<strong>为了防止上一层循环的<code>dp[0,...,j-1]</code>被覆盖，循环的时候 j 只能逆向枚举（正向会出现同一个物品被多次放入背包的情况，空间优化前没有这个限制）</strong>，伪代码为：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 01背包问题伪代码(空间优化版)</span><br><span class="line">dp[0,...,W] = 0</span><br><span class="line">for i = 1,...,N</span><br><span class="line">    for j = W,...,w[i] // 必须逆向枚举!!!</span><br><span class="line">        dp[j] = max(dp[j], dp[j−w[i]]+v[i])</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="完全背包"><a href="#完全背包" class="headerlink" title="完全背包"></a>完全背包</h2><p>完全背包（unbounded knapsack problem）与01背包不同就是每种物品可以有无限多个：一共有N种物品，每种物品有无限多个，第i（i从1开始）种物品的重量为w[i]，价值为v[i]。在总重量不超过背包承载上限W的情况下，能够装入背包的最大价值是多少？</p>
<p>完全背包本质就是0k背包。</p>
<h3 id="思路一"><a href="#思路一" class="headerlink" title="思路一"></a>思路一</h3><blockquote>
<p>ks(i,t) = max{ks(i-1, t - V[i] * k) + P[i] * k};  （0 &lt;= k * V[i] &lt;= t） ks(0,t)=0;  ks(i,0)=0;</p>
</blockquote>
<p>这种思路需要枚举每一个容量下的物品数量，优点是思考简单。</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 完全背包问题伪代码(空间优化版)</span><br><span class="line">dp[0,...,W] = 0</span><br><span class="line">for i = 1,...,N</span><br><span class="line">    for j = W,...,w[i] // 必须逆向枚举!!!</span><br><span class="line">        for k = [0, 1,..., j/w[i]]</span><br><span class="line">            dp[j] = max(dp[j], dp[j−k*w[i]]+k*v[i])</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="思路二"><a href="#思路二" class="headerlink" title="思路二"></a>思路二</h3><p>和01背包问题类似，也可进行空间优化，优化后不同点在于这里的 j 只能<strong>正向枚举（相当于一个物品重复使用了多次）</strong>而01背包只能逆向枚举，因为这里的max第二项是<code>dp[i]</code>而01背包是<code>dp[i-1]</code>，即这里就是需要覆盖而01背包需要避免覆盖。所以伪代码如下：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 完全背包问题思路一伪代码(空间优化版)</span><br><span class="line">dp[0,...,W] = 0</span><br><span class="line">for i = 1,...,N</span><br><span class="line">    for j = w[i],...,W // 必须正向枚举!!!</span><br><span class="line">        dp[j] = max(dp[j], dp[j−w[i]]+v[i])</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="多重背包"><a href="#多重背包" class="headerlink" title="多重背包"></a>多重背包</h2><p>多重背包（bounded knapsack problem）与前面不同就是每种物品是有限个：一共有N种物品，第i（i从1开始）种物品的数量为n[i]，重量为w[i]，价值为v[i]。在总重量不超过背包承载上限W的情况下，能够装入背包的最大价值是多少？</p>
<p>多重背包本质就是0k背包。</p>
<p>此时的分析和完全背包思路一差不多，也是从装入第 i 种物品多少件出发：装入第i种物品0件、1件、…n[i]件（还要满足不超过限重）。所以状态方程为：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># k为装入第i种物品的件数, k &lt;= min(n[i], j/w[i])</span><br><span class="line">dp[i][j] = max&#123;(dp[i-1][j − k*w[i]] + k*v[i]) for every k&#125;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>同理也可以进行空间优化，而且 j 也必须<strong>逆向枚举</strong>，优化后伪代码为</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 完全背包问题思路二伪代码(空间优化版)</span><br><span class="line">dp[0,...,W] = 0</span><br><span class="line">for i = 1,...,N</span><br><span class="line">    for j = W,...,w[i] // 必须逆向枚举!!!</span><br><span class="line">        for k = [0, 1,..., min(n[i], j/w[i])]</span><br><span class="line">            dp[j] = max(dp[j], dp[j−k*w[i]]+k*v[i])</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="恰好装满"><a href="#恰好装满" class="headerlink" title="恰好装满"></a>恰好装满</h2><p>恰好装满一开始将容量为0的dp[]初始化为0，其他初始化为-inf</p>
<p>常规背包则一开始都初始化为0</p>
<h2 id="求方案总数"><a href="#求方案总数" class="headerlink" title="求方案总数"></a>求方案总数</h2><p>将max函数换为sum函数</p>
<h2 id="求二维背包"><a href="#求二维背包" class="headerlink" title="求二维背包"></a>求二维背包</h2><p>多一层循环，其他无差别</p>
<h2 id="求最优方案"><a href="#求最优方案" class="headerlink" title="求最优方案"></a>求最优方案</h2><p>一般而言，背包问题是要求一个最优值，如果要求输出这个最优值的方案，可以参照一般动态规划问题输出方案的方法：记录下每个状态的最优值是由哪一个策略推出来的，这样便可根据这条策略找到上一个状态，从上一个状态接着向前推即可。</p>
<p>以01背包为例，我们可以再用一个数组G[i][j]来记录方案，设 <code>G[i][j] = 0</code>表示计算 dp[i][j] 的值时是采用了max中的前一项(也即dp[i−1][j])，<code>G[i][j] = 1</code> 表示采用了方程的后一项。即分别表示了两种策略: 未装入第 i 个物品及装了第 i 个物品。其实我们也可以直接从求好的dp[i][j]反推方案：若 <code>dp[i][j] = dp[i−1][j]</code> 说明未选第i个物品，反之说明选了。</p>
<h2 id="LeetCode题目"><a href="#LeetCode题目" class="headerlink" title="LeetCode题目"></a>LeetCode题目</h2><h4 id="416-分割等和子集"><a href="#416-分割等和子集" class="headerlink" title="416. 分割等和子集"></a><a href="https://leetcode-cn.com/problems/partition-equal-subset-sum/">416. 分割等和子集</a></h4><p>难度：太水辣</p>
<p>思路：构造一个二分之一数组和的背包，相当于“恰好填满”类背包问题。</p>
<h4 id="494-目标和"><a href="#494-目标和" class="headerlink" title="494. 目标和"></a><a href="https://leetcode-cn.com/problems/target-sum/">494. 目标和</a></h4><p>难度：水</p>
<p>思路：除了最好想的dfs，此题还可以用背包的思想来解决。</p>
<img src="https://i.loli.net/2021/06/19/Wa3o5YEAfFRze7U.png" alt="image.png" style="zoom:50%;" />

<h4 id="474-一和零"><a href="#474-一和零" class="headerlink" title="474. 一和零"></a><a href="https://leetcode-cn.com/problems/ones-and-zeroes/">474. 一和零</a></h4><p>难度：水</p>
<p>思路：二维背包，基础题</p>
<h4 id="322-零钱兑换"><a href="#322-零钱兑换" class="headerlink" title="322. 零钱兑换"></a><a href="https://leetcode-cn.com/problems/coin-change/">322. 零钱兑换</a></h4><p>难度：水</p>
<p>思路：完全背包，基础题，注意<strong>正向枚举</strong></p>
<h4 id="518-零钱兑换-II"><a href="#518-零钱兑换-II" class="headerlink" title="518. 零钱兑换 II"></a><a href="https://leetcode-cn.com/problems/coin-change-2/">518. 零钱兑换 II</a></h4><p>难度：水</p>
<p>思路：完全背包，基础题，注意初始化</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title>排序</title>
    <url>/2021/05/31/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<img src="https://i.loli.net/2021/07/28/9j2c8MsQCd7GhqW.png" alt="image.png" style="zoom:80%;" />

<h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><p><img src="https://i.loli.net/2021/07/28/58muPeCEJsRXvao.gif" alt="v2-62b35f38b9806cb2e50c7e4ee0ec3386_720w.gif"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BubbleSort</span><span class="params">(ElemType A[],<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n<span class="number">-1</span>;i++)&#123;</span><br><span class="line">    flag=<span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">for</span>(j=n<span class="number">-1</span>;j&gt;i;j--)&#123;</span><br><span class="line">      <span class="keyword">if</span>(A[j<span class="number">-1</span>]&gt;A[j])&#123;</span><br><span class="line">        <span class="built_in">swap</span>(A[j<span class="number">-1</span>],A[j]);</span><br><span class="line">        flag=<span class="literal">true</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(flag==<span class="literal">false</span>)</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="选择排序法"><a href="#选择排序法" class="headerlink" title="选择排序法"></a>选择排序法</h2><p><img src="https://i.loli.net/2021/07/28/rxCE7jPqwzkJsvi.gif" alt="v2-c5e176ffc200c8f4f137e732fe860b60_720w.gif"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">SelectSort</span><span class="params">(ElemType A[],<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n<span class="number">-1</span>;i++)&#123;</span><br><span class="line">    min=i;</span><br><span class="line">    <span class="keyword">for</span>(j=i+<span class="number">1</span>;j&lt;n;j++)</span><br><span class="line">      <span class="keyword">if</span>(A[j]&lt;A[min]) min=j;</span><br><span class="line">    <span class="keyword">if</span>(min!=i) </span><br><span class="line">      <span class="built_in">swap</span>(A[i],A[min]);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="插入排序法"><a href="#插入排序法" class="headerlink" title="插入排序法"></a>插入排序法</h2><p><img src="https://i.loli.net/2021/07/28/9k8UFYowPH4Q2jD.gif" alt="v2-38d6f9c006e3fdb24ffd82a58c39ca92_720w.gif"></p>
<h3 id="直接插入法"><a href="#直接插入法" class="headerlink" title="直接插入法"></a>直接插入法</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InsertSort</span><span class="params">(ElemType A[],<span class="keyword">int</span> n)</span></span>&#123;	</span><br><span class="line">  <span class="keyword">int</span> i,j;</span><br><span class="line">  <span class="keyword">for</span>(i=<span class="number">2</span>;i&lt;=n;i++)&#123;</span><br><span class="line">    A[<span class="number">0</span>]=A[i];</span><br><span class="line">    <span class="keyword">for</span>(j=i<span class="number">-1</span>;A[<span class="number">0</span>]&lt;A[j];--j)&#123;</span><br><span class="line">      A[j+<span class="number">1</span>]=A[j];</span><br><span class="line">    &#125;</span><br><span class="line">    A[j+<span class="number">1</span>]=A[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="折半插入法"><a href="#折半插入法" class="headerlink" title="折半插入法"></a>折半插入法</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InsertSort</span><span class="params">(ElemType A[],<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i,j,low,high,mid;</span><br><span class="line">  <span class="keyword">for</span>(i=<span class="number">2</span>;i&lt;=n;i++)&#123;</span><br><span class="line">    A[<span class="number">0</span>]=A[i];</span><br><span class="line">    low=<span class="number">1</span>;high=i<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">while</span>(low&lt;=high)&#123;</span><br><span class="line">      mid=(low+high)/<span class="number">2</span>;</span><br><span class="line">      <span class="keyword">if</span>(A[mid]&gt;A[<span class="number">0</span>]) high=mid<span class="number">-1</span>;</span><br><span class="line">      <span class="keyword">else</span> low=mid+<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(j=i<span class="number">-1</span>;j&gt;=high+<span class="number">1</span>;--j)</span><br><span class="line">      A[j+<span class="number">1</span>]=A[j];</span><br><span class="line">    A[high+<span class="number">1</span>]=A[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h2 id="希尔排序法-缩小增量排序"><a href="#希尔排序法-缩小增量排序" class="headerlink" title="希尔排序法/缩小增量排序"></a>希尔排序法/缩小增量排序</h2><p><img src="https://i.loli.net/2021/07/28/avz8JiowZNMb6Ck.gif" alt="v2-924c253a0ce18b65eb5d590699c8773b_720w.gif"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShellSort</span><span class="params">(ElemType A[],<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(dk=n/<span class="number">2</span>;dk&gt;=<span class="number">1</span>;dk/=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span>(i=dk+<span class="number">1</span>;i&lt;=n;++i)</span><br><span class="line">      <span class="keyword">if</span>(A[i]&lt;A[i-dk])&#123;</span><br><span class="line">        A[<span class="number">0</span>]=A[i];</span><br><span class="line">        <span class="keyword">for</span>(j=i-dk;j&gt;<span class="number">0</span>&amp;&amp;A[<span class="number">0</span>]&lt;A[j];j-=dk)</span><br><span class="line">          A[j+dk]=A[j];</span><br><span class="line">        A[j+dk]=A[<span class="number">0</span>];</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><p><img src="https://i.loli.net/2021/07/28/OgrIWpRxiEhLzP2.gif" alt="v2-ed06651ca489ff4454e9889ca0d753db_720w.gif"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">ElemType *B=(ElemType *)<span class="built_in">malloc</span>((n+<span class="number">1</span>)*<span class="built_in"><span class="keyword">sizeof</span></span>(ElemType));</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Merge</span><span class="params">(ElemType A[],<span class="keyword">int</span> low,<span class="keyword">int</span> mid,<span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> k=low;k&lt;=high;k++)</span><br><span class="line">    B[k]=A[k];</span><br><span class="line">  <span class="keyword">for</span>(i=low,j=mid+<span class="number">1</span>,k=i;i&lt;=mid&amp;&amp;j&lt;=high;k++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(B[i]&lt;=B[j])</span><br><span class="line">      A[k]=B[i++];</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      A[k]=B[j++];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span>(i&lt;=mid) A[k++]=B[i++];</span><br><span class="line">  <span class="keyword">while</span>(j&lt;=high) A[k++]=B[j++];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MergeSort</span><span class="params">(ElemType A[],<span class="keyword">int</span> low,<span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(low&lt;high)&#123;</span><br><span class="line">    <span class="keyword">int</span> mid=(low+high)/<span class="number">2</span>;</span><br><span class="line">    <span class="built_in">MergeSort</span>(A,low,mid);</span><br><span class="line">    <span class="built_in">MergeSort</span>(A,mid+<span class="number">1.</span>high);</span><br><span class="line">    <span class="built_in">Merge</span>(A,low,mid,high);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="快速排序法"><a href="#快速排序法" class="headerlink" title="快速排序法"></a>快速排序法</h2><p><img src="https://i.loli.net/2021/07/28/EiQ9IC1hTcsSdOX.gif" alt="v2-e30a1dcb54af5e222243d8053d3a5a0d_720w.gif"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">QuickSort</span><span class="params">(ElemType A[],<span class="keyword">int</span> low,<span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(low&lt;high)&#123;</span><br><span class="line">    <span class="keyword">int</span> pivotpos=<span class="built_in">Partition</span>(A,low,high);</span><br><span class="line">    <span class="built_in">QuickSort</span>(A,low,pivotpos<span class="number">-1</span>);</span><br><span class="line">    <span class="built_in">QuickSort</span>(A,pivot+<span class="number">1</span>,high);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Partition</span><span class="params">(ElemType A[],<span class="keyword">int</span> low,<span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">  ElemType pivot=A[low];</span><br><span class="line">  <span class="keyword">while</span>(low&lt;high)&#123;</span><br><span class="line">    <span class="keyword">while</span>(low&lt;high&amp;&amp;A[high]&gt;=pivot) --high;</span><br><span class="line">    A[low]=A[high];</span><br><span class="line">    <span class="keyword">while</span>(low&lt;high&amp;&amp;A[low]&lt;=pivot) ++low;</span><br><span class="line">    A[high]=A[low];</span><br><span class="line">  &#125;</span><br><span class="line">  A[low]=pivot;</span><br><span class="line">  <span class="keyword">return</span> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><p><img src="https://i.loli.net/2021/07/28/vS56eJ8OxuBEwhN.gif" alt="v2-c892078dcb61230c6233fc5de4b4aebc_720w.gif"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BuildMaxHeap</span><span class="params">(ElemType A[],<span class="keyword">int</span> len)</span></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=len/<span class="number">2</span>;i&gt;<span class="number">0</span>;i--)</span><br><span class="line">    <span class="built_in">HeadAdjust</span>(A,i,len);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">HeadAdjust</span><span class="params">(ElemType A[],<span class="keyword">int</span> k,<span class="keyword">int</span> len)</span></span>&#123;</span><br><span class="line">  A[<span class="number">0</span>]=A[k];</span><br><span class="line">  <span class="keyword">for</span>(i=<span class="number">2</span>*k;i&lt;=len;i*=<span class="number">2</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;len&amp;&amp;A[i]&lt;A[i+<span class="number">1</span>])</span><br><span class="line">      i++;</span><br><span class="line">    <span class="keyword">if</span>(A[<span class="number">0</span>]&gt;A[i]) <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">      A[k]=A[i];</span><br><span class="line">      k=i;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  A[k]=A[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">HeapSort</span><span class="params">(Elemtype A[],<span class="keyword">int</span> len)</span></span>&#123;</span><br><span class="line">  <span class="built_in">BuildMaxHeap</span>(A,len);</span><br><span class="line">  <span class="keyword">for</span>(i=len;i&gt;<span class="number">1</span>;i--)&#123;</span><br><span class="line">    <span class="built_in">Swap</span>(A[i],A[<span class="number">1</span>]);</span><br><span class="line">    <span class="built_in">HeadAdjust</span>(A,<span class="number">1</span>,i<span class="number">-1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h2><p><img src="https://i.loli.net/2021/07/28/HE2DU8sf5Z1WOLg.gif" alt="v2-86055f604a687cc18e781160bcc8b781_720w.gif"></p>
<h2 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h2><p><img src="https://i.loli.net/2021/07/28/liCLHKy2Tgoc8aG.gif" alt="v2-ab04919ee2dbadd84ad065760b8eb1c9_720w.gif"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> exp = <span class="number">1</span>;	<span class="comment">//exp表示排到第几位了</span></span><br><span class="line"><span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">buf</span><span class="params">(n)</span></span>;</span><br><span class="line"><span class="keyword">int</span> maxVal = *<span class="built_in">max_element</span>(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (maxVal &gt;= exp) &#123;</span><br><span class="line">  <span class="function">vector&lt;<span class="keyword">int</span>&gt; <span class="title">cnt</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">    <span class="keyword">int</span> digit = (nums[i] / exp) % <span class="number">10</span>;</span><br><span class="line">    cnt[digit]++;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    cnt[i] += cnt[i - <span class="number">1</span>];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;	<span class="comment">//注意是反着来恢复连接的</span></span><br><span class="line">    <span class="keyword">int</span> digit = (nums[i] / exp) % <span class="number">10</span>;</span><br><span class="line">    buf[cnt[digit] - <span class="number">1</span>] = nums[i];</span><br><span class="line">    cnt[digit]--;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">copy</span>(buf.<span class="built_in">begin</span>(), buf.<span class="built_in">end</span>(), nums.<span class="built_in">begin</span>());</span><br><span class="line">  exp *= <span class="number">10</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h2><img src="https://i.loli.net/2021/07/28/gcZVTyI7Uu2w35x.gif" alt="v2-126c359bd98e64c79a2a1db2e829e57b_720w.gif" style="zoom:100%;" />

]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2021/11/26/%E8%A6%81%E5%AD%A6%E7%9A%84%E5%86%85%E5%AE%B9/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/u011412768/article/details/108015783">https://blog.csdn.net/u011412768/article/details/108015783</a> BERT的结构和原理实现</p>
<p>Bert的pool层是咋回事？</p>
]]></content>
  </entry>
  <entry>
    <title>李宏毅BERT和他的朋友们</title>
    <url>/2021/11/23/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP/%E6%9D%8E%E5%AE%8F%E6%AF%85BERT%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/</url>
    <content><![CDATA[<h1 id="BERT和他的朋友们"><a href="#BERT和他的朋友们" class="headerlink" title="BERT和他的朋友们"></a>BERT和他的朋友们</h1><h2 id="过去的历史"><a href="#过去的历史" class="headerlink" title="过去的历史"></a>过去的历史</h2><p>One-hot编码，缺点是：词意相近的词在one-hot中并没有体现出“相近”</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpcjs2zo9j30qy0qimyf.jpg" alt="image-20211123201633634" style="zoom:25%;" />

<p>使用word embedding，将one-hot和word class联合起来做投影，增加了语义信息</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpcmk2q1kj315g0u0ad4.jpg" alt="image-20211123201916084" style="zoom: 33%;" />

<p>但是还是解决不了一词多义的情况。</p>
<p>现在的Model可以做到每一个word token（一个词的不同意思）有一个word embedding,而不是一个word type（同一个词）共用所有word embedding。所以被称为contextualized word embedding.</p>
<h2 id="ELMO"><a href="#ELMO" class="headerlink" title="ELMO"></a>ELMO</h2><p><strong>Embeddings from Language Model</strong></p>
<ul>
<li>RNN based language model (trained from lots of sentences)</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpcxfiwzrj31ir0u0dkd.jpg" alt="image-20211123202942843" style="zoom: 33%;" />

<p>EMLO Model是使用RNN网络的，吃“&lt;BOS&gt;”预测“潮水”，吃“潮水”预测“退了”。但是这样只能从前预测后，所以改进成双向的。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpdaonv6sj31rg0rgwii.jpg" alt="image-20211123204227809" style="zoom: 25%;" />

<p>双向的网络完成后，还可以增加RNN的深度，这样就会形成很多embedding（红色方框里的部分，可以看出一个双向RNN构成的embedding可能只是把两个方向形成的embdding简单叠加，称为串接），ELMO表示：我全都要！</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpdc8wb41j31gj0u0tdm.jpg" alt="image-20211123204357708" style="zoom:25%;" />

<p>那么如何全都要呢？可以用$\alpha_1、\alpha_2$这些参数构成，比如一个有两层的ELMO Model，我们就用$\alpha_1、\alpha_2$构成新的蓝色的word embedding，其中$\alpha_1、\alpha_2$是根据下游任务学习出来的参数。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpdj9o4q0j31470u041w.jpg" alt="image-20211123205042354" style="zoom:33%;" />

<p>比如SQuAd任务就需要第一层占很大的比例，如上图所示。</p>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>Bidirectional Encoder Representations from Transformers</p>
<h3 id="Training-of-BERT"><a href="#Training-of-BERT" class="headerlink" title="Training of BERT"></a>Training of BERT</h3><ol>
<li>Masked LM: 随机MASK掉15%的token，然后让BERT去猜</li>
</ol>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpdu60619j31950u0whq.jpg" alt="image-20211123210110862" style="zoom:33%;" />

<p>被MASK掉的词经过一个Linear Multi-class Classifier就可以生成真正的词，而且BERT层数很少，所以就要求word的representation很准确。（大概是这样，没太听懂…）</p>
<ol start="2">
<li>Next Sentence Prediction</li>
</ol>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpe75bx0ij31970u0goi.jpg" alt="image-20211123211339931" style="zoom: 33%;" />

<p>[SEP]代表分段标志</p>
<p>[CLS]代表classify标志，它也是一个向量，经过一个linear binary classifier可以输出后面的两句话是否联系。</p>
<p>那么[CLS]为什么在句首而不是句尾呢？如果是ELMO的RNN，[CLS]确实应该放在句尾，但是BERT使用的是self-attention机制，这个机制就是“天涯若比邻”，两个距离很近的word和距离很远的word对BERT来说是一样的。所以对于BERT来说放在句首或者句尾是一样的。</p>
<p>这个linear binary classifier和BERT是一起训练的，在训练过程中，BERT也学到了参数。</p>
<p>Approaches 1 and 2 are used at the same time.</p>
<h2 id="How-to-use-BERT"><a href="#How-to-use-BERT" class="headerlink" title="How to use BERT?"></a>How to use BERT?</h2><p>BERT除了可以像ELMO一样，输出每个word的embedding。更多的利用是和下游任务结合一起做训练（fine-tune）。</p>
<h3 id="Case1"><a href="#Case1" class="headerlink" title="Case1"></a>Case1</h3><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpepigzmbj31cy0u0dir.jpg" alt="image-20211123213118894" style="zoom:33%;" />

<p>类比上面的NSP任务，在句子前面加一个分类的标志，然后将该标志（图里就是[CLS]）放入一个linear classifier里，得出句子的分类。然后用样本同时训练classifier和BERT，对于BERT当然只是fine-tune。</p>
<h3 id="Case2"><a href="#Case2" class="headerlink" title="Case2"></a>Case2</h3><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpewbb72aj31dq0u0whq.jpg" alt="image-20211123213750678" style="zoom:33%;" />

<p>这个任务类似于sequence labeling，相当于把每一个input的word分一下类。可以给每个输出的embedding后面接一个分类器，然后一起训练。</p>
<h3 id="Case3"><a href="#Case3" class="headerlink" title="Case3"></a>Case3</h3><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpey18ipcj31cx0u0adi.jpg" alt="image-20211123213929816" style="zoom:33%;" />

<p>这个任务是输入两个句子，输出一个分类。比如NLI任务，一个句子是前提，一个句子是假设，让Model去输出推论（包括对T、错F、不知道Unknown）。</p>
<p>一个[CLS]拿去做分类，判断是T、F还是Unknown，一个[SEP]标志用来分开两个句子。</p>
<h3 id="Case4"><a href="#Case4" class="headerlink" title="Case4"></a>Case4</h3><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpf3hiyokj31hl0u043z.jpg" alt="image-20211123214444422" style="zoom:33%;" />

<p>这个任务QA任务，是Extraction-based的QA，即问题的答案在文中都有，只需要输出是哪几个单词即可。那么如何做呢？</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpf4y7bw6j31cm0u0gos.jpg" alt="image-20211123214608845" style="zoom:33%;" />

<p>我们有两个向量（图中的红色和蓝竖条，是学出来的），对于每个单词输出的embedding，我们和红色的竖条所代表的向量进行dot-product，然后再softmax，最大的代表最可能的起始位置，蓝色同理代表最可能的结束位置。</p>
<p>另外，如果结束位置在开始位置前面，代表此题无答案。</p>
<h2 id="ERNIE"><a href="#ERNIE" class="headerlink" title="ERNIE"></a>ERNIE</h2><p><strong>Enhanced Rrepresentation through Knowledge Integration</strong></p>
<ul>
<li>是针对中文提出的Model</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpfai90d5j31hi0u0tdl.jpg" alt="image-20211123215129185" style="zoom:25%;" />

<p>MASK掉的是中文里的词，这样会更有难度，训练出更好地模型。</p>
<h2 id="What-does-BERT-learn"><a href="#What-does-BERT-learn" class="headerlink" title="What does BERT learn?"></a>What does BERT learn?</h2><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpfby8vnoj317f0u0dky.jpg" alt="image-20211123215252672" style="zoom:33%;" />

<p>BERT一共24层，每一层学到的东西不同，就像是NLP的pipeline，比如最底层学到的是词汇的词性，然后是整个剧的文法，然后可以找出每个代词可以代表什么。高层做一些更困难的任务。</p>
<p>上图是使用ELMO的对每层输出的embedding进行加权做不同下游任务的图片，右边的列表中，蓝色的竖条越大，代表该层embedding所占的比重在最终的embedding中越大。</p>
<h2 id="Multilingual-BERT"><a href="#Multilingual-BERT" class="headerlink" title="Multilingual BERT"></a>Multilingual BERT</h2><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpfiq3oizj31as0u0djq.jpg" alt="image-20211123215923030" style="zoom:33%;" />

<p>在用104种语言训练了BERT以后，BERT似乎自己学会了各个语言间的对应关系。当我们用英文的data训练BERT的分类任务后，他（104 languages训练的）似乎学会了给中文进行分类。</p>
<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p><strong>Generative Pre-Training</strong></p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpi9240d4j317t0u00vt.jpg" alt="image-20211123233353022" style="zoom:33%;" />

<ul>
<li>这是一个硕大无敌的language model</li>
<li>GPT用的是Transformer的Decoder</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpiaeq6iqj30xc0maq5c.jpg" alt="image-20211123233511933" style="zoom:25%;" />

<p>GPT是如何训练的呢？也是通过预测下一个词（仅是从左到右预测）。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpibzxqs7j30un0u0ac5.jpg" alt="image-20211123233643241" style="zoom:33%;" />

<p>GPT由于很庞大，有了很多匪夷所思的能力，都是zero-shot的，直接就可以使用。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwpiec7570j31og0tawij.jpg" alt="image-20211123233857985" style="zoom:33%;" />

<p>以Reading Comprehension为例，输入完问题，打一个“A:”，就能直接输出答案。（可能是在文章里有“A:”，所以GPT-2学会了“A:”后面接答案）</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>BERT</tag>
        <tag>ELMO</tag>
        <tag>GPT</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅NLP1:Speech Recgnition</title>
    <url>/2021/10/25/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP1:Speech%20Recgnition/</url>
    <content><![CDATA[<h1 id="Speech-Recognition"><a href="#Speech-Recognition" class="headerlink" title="Speech Recognition"></a>Speech Recognition</h1><h2 id="Chapter-One"><a href="#Chapter-One" class="headerlink" title="Chapter One"></a>Chapter One</h2><h3 id="Speech2Text"><a href="#Speech2Text" class="headerlink" title="Speech2Text"></a>Speech2Text</h3><p>Speech: a sequence of vector (length T, dimension d)</p>
<p>Text: a sequence of token(length N, V different tokens)</p>
<p>Usually, T &gt; N </p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjes485ej61lo0jo76302.jpg" alt="image-20211025105336856" style="zoom: 33%;" />

<p>So, what is this so-called “Token”?</p>
<p>Token= Phoneme（音素、音位）/Grapheme （字素、字母）/Morpheme（词素、形态素：意思的最小单位）/Words/ Bytes字节</p>
<p>Lexicon（词典、辞典）: word to phonemes</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjeuyz1bj613v0u077802.jpg" alt="image-20211025111923860" style="zoom: 33%;" />

<p>英文的字母超过26个，因为起码要包含空格，可能还包含标点符号。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjeww5bxj61460u0mzn02.jpg" alt="image-20211025113011276" style="zoom:33%;" />

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjero4m1j61450u040f02.jpg" alt="image-20211025113305303" style="zoom:33%;" />

<p>语音辨识的大趋势：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjewgb3dj61ek0u040202.jpg" alt="image-20211025113522667" style="zoom:25%;" />

<h3 id="Acoustic-Feature"><a href="#Acoustic-Feature" class="headerlink" title="Acoustic Feature"></a>Acoustic Feature</h3><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjeu409yj61420u0tbk02.jpg" alt="image-20211025114523806" style="zoom:33%;" />

<p>一般窗口（window）大小为25ms，采样得到1个frame。每次移动10ms，所以会有重复的部分，1s的语音可以得到100个frame。</p>
<p>对于16KHz的声音，25ms（1s=1000ms）一个sample，1s有400sample。</p>
<p> **MFCC(Mel-frequency cepstral coefficients)**：梅尔频率倒谱系数。梅尔频率是基于人耳听觉特性提出来的，它与Hz频率成非线性对应关系。梅尔频率倒谱系数(MFCC)则是利用它们之间的这种关系，计算得到的Hz频谱特征。主要用于语音数据特征提取和降低运算维度。例如：对于一帧有512维(采样点)数据，经过MFCC后可以提取出最重要的40维(一般而言)数据同时也达到了将维的目的。MFCC一般会经过这么几个步骤：预加重，分帧，加窗，快速傅里叶变换(FFT)，梅尔滤波器组，离散余弦变换(DCT).其中最重要的就是FFT和梅尔滤波器组，这两个进行了主要的将维操作。</p>
<p>现在更多使用<strong>filter bank</strong>得出MFCC。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjevoq4lj61420u0jto02.jpg" alt="image-20211025130310484" style="zoom:33%;" />

<p>spectrogram：声谱图</p>
<p>filter bank：由哺乳动物器官</p>
<p>各种技术的应用情况：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjer1idej61lo0jo76302.jpg" alt="image-20211025132015476" style="zoom: 25%;" />

<p>How much data do we need (English corpora) ?</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjeswa62j61ih0u0n1j02.jpg" alt="image-20211025133030622" style="zoom: 33%;" /> 

<p>粉色方框内是常用的语音库，其中TIMIT之于声音就相当于MINST相当于计算机视觉，它是一个很小的库，可以方便初步检测你的构想。</p>
<p>将MINST和CIFAR-20投影到图中还在TIMIIT之下。</p>
<p>谷歌等公司的训练数据很大，比这里显示的还大20倍以上。</p>
<h3 id="Two-points-of-view"><a href="#Two-points-of-view" class="headerlink" title="Two points of view"></a>Two points of view</h3><ul>
<li>Seq2Seq</li>
<li>HMM</li>
</ul>
<p>19年的趋势：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjetm9lpj61f20tyq4q02.jpg" alt="image-20211025141709059" style="zoom:25%;" />

<h2 id="Chapter-Two"><a href="#Chapter-Two" class="headerlink" title="Chapter Two"></a>Chapter Two</h2><h3 id="LAS-Listen-Attend-and-Spell"><a href="#LAS-Listen-Attend-and-Spell" class="headerlink" title="LAS(Listen, Attend, and Spell)"></a>LAS(Listen, Attend, and Spell)</h3><p>It is the typical seq2seq with attention.</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjeuk7slj61290u0mzm02.jpg" alt="image-20211025142234784" style="zoom: 33%;" />

<h3 id="Listen"><a href="#Listen" class="headerlink" title="Listen"></a>Listen</h3><p>Listen means Encoder!</p>
<p>Encoder可以采用RNN、CNN、CNN+RNN、以及自注意模型：</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjkp1r12j61290u0ac502.jpg" alt="image-20211025143028343" style="zoom:25%;" /><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjmm883bj61460u0dir02.jpg" alt="image-20211025143219587" style="zoom:25%;" /></p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjnzu48rj61360u00wd02.jpg" alt="image-20211025143338379" style="zoom:25%;" />

<h4 id="Down-Sampling"><a href="#Down-Sampling" class="headerlink" title="Down Sampling"></a>Down Sampling</h4><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjt18ne2j612y0u0ju402.jpg" alt="image-20211025143829255" style="zoom:33%;" />

<ul>
<li><p>Pyramid RNN: 将两个综合起来形成一个</p>
</li>
<li><p>Pooling over time: 两个里面选一个</p>
</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrjwx9bm8j61590u042r02.jpg" alt="image-20211025144212858" style="zoom: 33%;" />

<ul>
<li>TDNN: 和Dilated CNN一样，只是名字不同，4个里面只保留第一个和最后一个</li>
<li>Truncated Self-attention:只看附近的几个，不然数据太大，照顾不过来</li>
</ul>
<h3 id="Attend"><a href="#Attend" class="headerlink" title="Attend"></a>Attend</h3><p>Attention实现方式1：h和z分别乘线性变换矩阵，然后点积求<em><strong>相似度</strong></em>。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrkmdgnbfj615a0u0dif02.jpg" alt="image-20211025150640623" style="zoom:33%;" />

<p>Attention实现方式2：加和，tanh，线性变换</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrkol14irj61120u0tb602.jpg" alt="image-20211025150848599" style="zoom:33%;" />

<h3 id="Spell"><a href="#Spell" class="headerlink" title="Spell"></a>Spell</h3><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrl18mulyj614a0u0gom02.jpg" alt="image-20211025152058278" style="zoom:33%;" />

<p>Spell就是decoder！</p>
<p>hidden state使用的是RNN网络；$C^0$是encoder网络的输出，也是decoder网络的输入，叫做context vector上下文；根据想要输出的Token的不同，设置不同的输出softmax，这里想以字母为输出单位，输入的是猫，所以第一个字母输出的是“c”，同时softmax的Size V就是26（不考虑空格）。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrl8jnrnrj614t0u041h02.jpg" alt="image-20211025152759773" style="zoom:33%;" />

<p>$z^1$再去做Attention，生成$c^1$；在RNN网络中，第一个生成的字母“c”和encoder生成的$c^1$一起生成“cat”的第二个字母“a”。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrlcsycu8j614t0u0acx02.jpg" alt="image-20211025153204616" style="zoom:33%;" />

<p>最终生成结果如上图，生成了&lt;EOS&gt;所以结束，搜索的时候可以使用束搜索。</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrlrpgtnjj610p0u041q02.jpg" alt="image-20211025154624645" style="zoom:33%;" />

<p>正确输出表示为one-hot向量，遂使用交叉熵cross entropy来训练网络。</p>
<h4 id="Teacher-Forcing"><a href="#Teacher-Forcing" class="headerlink" title="Teacher Forcing"></a>Teacher Forcing</h4><p>对于本例子的第二个输出字母“a”，它的输出与第一个字母“c”是相关的，但是在训练网络时，我们假设传入第二个RNN的就是“c”的one-hot向量，也就是Use ground truth了。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrlu6v9yzj61310u041m02.jpg" alt="image-20211025154848077" style="zoom:33%;" />

<p>为什么不能用前一个的输出进行训练呢？主要是因为参数一开始是随机的，这样第一个参数训练好以后，可能第二个网络就白训练了，它的系数直接作废。如下图：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrmdeei8gj614f0u0q6102.jpg" alt="image-20211025160715074" style="zoom:33%;" />

<h4 id="Back-to-Attend"><a href="#Back-to-Attend" class="headerlink" title="Back to Attend"></a>Back to Attend</h4><p>Attention有两种方法：一是生成出的Attend用于下次，也可以用于本次。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrmg75rgwj314f0u0jts.jpg" alt="image-20211025160956574" style="zoom:33%;" />

<p>还有一种生成法是既用于本次也用于下次。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrmhmofacj314f0u0jti.jpg" alt="image-20211025161119032" style="zoom:33%;" />

<h4 id="Location-aware-attention"><a href="#Location-aware-attention" class="headerlink" title="Location-aware attention"></a>Location-aware attention</h4><p>防止Attend乱跳，生成第二个token的时候要考虑第一个token生成的$\alpha$。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvrnbvqdy9j316y0u0acw.jpg" alt="image-20211025164023699" style="zoom:33%;" />

<h4 id="Limition-of-LSA"><a href="#Limition-of-LSA" class="headerlink" title="Limition of LSA"></a>Limition of LSA</h4><p>必须待语音全部输入完以后才能输出结果（全部声音序列添加进Encoder），不能边听边输出结果。</p>
<h2 id="Chapter-Three"><a href="#Chapter-Three" class="headerlink" title="Chapter Three"></a>Chapter Three</h2><h3 id="CTC"><a href="#CTC" class="headerlink" title="CTC"></a>CTC</h3><p>可以做on-line speech recgnization, 所以用uni-directional RNN.</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvsr78l668j311e0u0goo.jpg" alt="image-20211026153953482" style="zoom:33%;" />

<p>由于每个acoustic feature所带信息量太小，一般只有10ms，所以将token size V +1，即可以输出“空”，符号为$\phi$。待前面积累的acoustic feature可以输出token时再输出。</p>
<p>Input T acoustic feature, output T token, but outputs include $\phi$, remove $\phi$, merge duplicate tokens.</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvsrf2suwgj31m60kqdi4.jpg" alt="image-20211026154725420" style="zoom: 25%;" />

<h4 id="How-to-train"><a href="#How-to-train" class="headerlink" title="How to train"></a>How to train</h4><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvsrhstb4aj30zn0u041t.jpg" alt="image-20211026155002053" style="zoom:33%;" />

<p>需要对每一个frame标注正确的输出结果。那么如何标注呢？</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvt1j0ev83j31os0c2abp.jpg" alt="image-20211026213708688" style="zoom:25%;" />

<p>这种标注叫做alignment。手动地添加“空”。但是“空”的位置有很多种添加方式，选择哪种呢？CTC全都选择！</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvt1mceivuj311k0u0jw2.jpg" alt="image-20211026214024026" style="zoom:33%;" />

<p>CTC很容易“结巴”，原因如下所示，如果一旦中间出现一个空白，就会出现“结巴”。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtvh0wm2uj314d0u00wi.jpg" alt="image-20211027145314876" style="zoom:33%;" />

<h3 id="RNN-Transducer"><a href="#RNN-Transducer" class="headerlink" title="RNN-Transducer"></a>RNN-Transducer</h3><h4 id="RNA"><a href="#RNA" class="headerlink" title="RNA"></a>RNA</h4><p>RNA( Recurrent Neutral Aligner): 优化了CTC“结巴”的问题，在输出token时不再是独立的，而是使用RNN/ LSTM等手段，这样便不会“结巴”，但是对于一个frame输出多个token还没有很好的解决办法，比如输出“th”时，虽然发音是一个，但是要输出两个字母，如何解决？</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtvnxhsmwj315m0u041o.jpg" alt="image-20211027145953600" style="zoom:33%;" />

<h4 id="RNN-T"><a href="#RNN-T" class="headerlink" title="RNN-T"></a>RNN-T</h4><p>RNN-T很好的解决了一个frame输出多个token的问题。它对于一个frame会一直输出，知道出现“空”，“空”代表它需要下一个frame了。</p>
<p>所以，对于T个acoustic feature，有T个“空”。</p>
<p>但是RNN-T有alignment的问题，label有多种可能，RNN-T与CTC一样使用所有可能训练网络。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtvrhdfw3j315m0u0n0n.jpg" alt="image-20211027150318589" style="zoom:33%;" />

<p>这里再用一个Language Model（它只看token当作输入）在输出的token基础上生成传递给下一个网络的内容，遇到“空”就忽视。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtwf1abdkj31380u0goz.jpg" alt="image-20211027152556545" style="zoom:33%;" />

<h4 id="Neural-Transducer"><a href="#Neural-Transducer" class="headerlink" title="Neural Transducer"></a>Neural Transducer</h4><p>把多个acoustic feature合成一个window再使用Attention输出。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtwkp0isuj31440u077f.jpg" alt="image-20211027153123104" style="zoom:33%;" />

<h4 id="MoCha"><a href="#MoCha" class="headerlink" title="MoCha"></a>MoCha</h4><p>Monotonic Chunkwise Attention: 可以动态地自选窗口的位置（和大小）</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtwrnwvjij316l0u0wgo.jpg" alt="image-20211027153805384" style="zoom:33%;" />

<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtwtysys2j313h0u0n1l.jpg" alt="image-20211027154017919" style="zoom:50%;" />

































































]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Speech Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅NLP6:NLP tasks</title>
    <url>/2021/11/05/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP6:NLP%20tasks/</url>
    <content><![CDATA[<h1 id="NLP-tasks"><a href="#NLP-tasks" class="headerlink" title="NLP tasks"></a>NLP tasks</h1><h2 id="Chapter-One"><a href="#Chapter-One" class="headerlink" title="Chapter One"></a>Chapter One</h2><h3 id="Mutiple-Sentence"><a href="#Mutiple-Sentence" class="headerlink" title="Mutiple Sentence"></a>Mutiple Sentence</h3><p>对于多个句子的输入，我们该怎么办？</p>
<p>第一种方法：两个句子分别用Model，然后把它们Integrate起来，Integrate的时候可以使用Attention，也就是这个句子生成时参考另外一个句子。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6hytzvxxj30u40n0jt8.jpg" alt="image-20211107125817089" style="zoom:25%;" />

<p>第二种方法：把两个句子中间加一个[SEP]分隔开，然后扔到一个Model中。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6hzns2ntj30ua0kkjsi.jpg" alt="image-20211107125908286" style="zoom:25%;" />

<p>所有NLP Tasks：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6i1396r9j312z0u0tby.jpg" alt="image-20211107130029683" style="zoom:33%;" />

<h4 id="POS-Tagging"><a href="#POS-Tagging" class="headerlink" title="POS Tagging"></a>POS Tagging</h4><p>POS: Part of Speech</p>
<p>就是对一个句子进行词性标注</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6jq059vqj31hq0pead7.jpg" alt="image-20211107135901243" style="zoom:33%;" />

<h4 id="Word-Segmentation"><a href="#Word-Segmentation" class="headerlink" title="Word Segmentation"></a>Word Segmentation</h4><p>例如：台湾大学简称台大</p>
<p>输出N继续输出，输出Y就分词。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6jt5io6fj31dc0u0mzt.jpg" alt="image-20211107140203897" style="zoom:33%;" />

<h4 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h4><p>即做句法分析</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6k0187mbj31eo0u0jtt.jpg" alt="image-20211107140840717" style="zoom:33%;" />

<h4 id="Coreference-Resolution"><a href="#Coreference-Resolution" class="headerlink" title="Coreference Resolution"></a>Coreference Resolution</h4><p>指代消解：是指找到文章中相同的代词指代的内容。比如“he”、“she”指代的意思。顾名思义就是将指代词消解。</p>
<h4 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h4><ul>
<li><p>Extractive summarization: 用文章里的某些句子平凑成一个summary。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6plze1crj31lc0r2414.jpg" alt="image-20211107172246394" style="zoom: 25%;" /></li>
<li><p>Abstractive summarization：从文章中学习到summary</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6py36v0bj31mm0oqjtm.jpg" alt="image-20211107173424409" style="zoom:25%;" /></li>
</ul>
<h4 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h4><img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6r7k7mmjj319t0u0q5v.jpg" alt="image-20211107181801483" style="zoom: 33%;" />

<h4 id="Grammar-Error-Correction"><a href="#Grammar-Error-Correction" class="headerlink" title="Grammar Error Correction"></a>Grammar Error Correction</h4><p>检验句子里的语法错误：一种是使用Seq2Seq模型，一种是对于每个单词输出一种类型：比如R是替换，需要将本单词替换成别的单词。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6rdjsf51j318v0u079a.jpg" alt="image-20211107182348922" style="zoom:33%;" />

<h4 id="Sentiment-Classification"><a href="#Sentiment-Classification" class="headerlink" title="Sentiment Classification"></a>Sentiment Classification</h4><p>输入一段话，判断这段话的情感。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6rlm3vcxj31mw0u0ae3.jpg" alt="image-20211107183134331" style="zoom:33%;" />

<h4 id="Stance-Detection"><a href="#Stance-Detection" class="headerlink" title="Stance Detection"></a>Stance Detection</h4><p>判断Reply对Source的立场。</p>
<p>一般立场可以分为Support、Denying、Querying、Commenting，所以这类问题又被称为SDQC问题。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6rw74i7rj31k40i440p.jpg" alt="image-20211107184145104" style="zoom:33%;" />

<p>它可以用在Veracity Prediction里面。</p>
<h4 id="Veracity-Prediction"><a href="#Veracity-Prediction" class="headerlink" title="Veracity Prediction"></a>Veracity Prediction</h4><p>判断一段话是真的还是假的。</p>
<p>除了原post文，有时还要加上reply，有时还要加上维基百科的相关内容。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw6sc5tqymj317s0cmaat.jpg" alt="image-20211107185706521" style="zoom:33%;" />

<h4 id="NLI"><a href="#NLI" class="headerlink" title="NLI"></a>NLI</h4><p>NLI就是Natural Language Inference：根据一个premise，判断hypothesis。有contradiction（反对）、entailment（蕴含）、neutral（中性）。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7ltlr6a7j314s0u0dkt.jpg" alt="image-20211108115714188" style="zoom:33%;" />

<h4 id="Search-Engine"><a href="#Search-Engine" class="headerlink" title="Search Engine"></a>Search Engine</h4><p>判断搜索内容和一个网页内容的关联度，可以看做输入两个seq，输出一个class。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7m8of9d6j315v0u0juj.jpg" alt="image-20211108121144523" style="zoom:33%;" />

<p>BERT的厉害之处在于它可以识别同一个单词的不同含义。</p>
<h4 id="Question-Answering"><a href="#Question-Answering" class="headerlink" title="Question Answering"></a>Question Answering</h4><p>Extractive QA：大部分都是提取式的，也就是答案就在文章里。甚至直接输出两个数字，数字间的就是答案。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7n8tusbpj31gz0u0ten.jpg" alt="image-20211108124628759" style="zoom:33%;" />

<h4 id="Dialogue"><a href="#Dialogue" class="headerlink" title="Dialogue"></a>Dialogue</h4><ol>
<li><p>Chatting</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7nqm6rd2j318e0u0tbh.jpg" alt="image-20211108130335066" style="zoom:33%;" /></li>
<li><p>Task-oriented</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7ntxdyi0j31fm0u0n3p.jpg" alt="image-20211108130645887" style="zoom:33%;" />
因为是任务导向的，所以需要规定Model的Action：比如打招呼、入住日、退房日。

 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7nvz0ubkj318u0u044d.jpg" alt="image-20211108130843321" style="zoom:33%;" />

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7pjnsh16j31400u0ten.jpg" alt="image-20211108140604182" style="zoom:33%;" />

<h4 id="NLU"><a href="#NLU" class="headerlink" title="NLU"></a>NLU</h4><p>NLU就是Natural Language Understanding。</p>
<ul>
<li>Intent Classification：给一个seq，让机器将其分类。比如是关于什么事情的。</li>
<li>Slot Filling：输入一个seq，输出组成该seq的token的label。<img src="/Users/apple/Library/Application Support/typora-user-images/image-20211108141904012.png" alt="image-20211108141904012" style="zoom:33%;" /></li>
</ul>
<p>做一个语音输入语音输出的对话模组：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7pz5pz3wj31bm0u00va.jpg" alt="image-20211108142059438" style="zoom:33%;" />

<h4 id="Knowledge-Graph"><a href="#Knowledge-Graph" class="headerlink" title="Knowledge Graph"></a>Knowledge Graph</h4><p>知识图谱：包括entity和relation。</p>
<p>简化的来说就是：先提取实体，然后提取关系</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7q484grlj31370u0q6j.jpg" alt="image-20211108142550188" style="zoom:33%;" />

<h5 id="Name-Entity-Recognition"><a href="#Name-Entity-Recognition" class="headerlink" title="Name Entity Recognition"></a>Name Entity Recognition</h5><p>Name Entity: usually include: people、organizations、places。具体包括什么取决于具体的任务。</p>
<p>输入一个Seq，输出class for each token</p>
<h5 id="Relation-Extraction"><a href="#Relation-Extraction" class="headerlink" title="Relation Extraction"></a>Relation Extraction</h5><p>将原文和想要提取出关系的两个实体输入进去，输出他们的关系。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7s7uas11j31po0joacc.jpg" alt="image-20211108153832836" style="zoom:25%;" />

<h3 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h3><p>GLUE：General Language Understanding Evaluation</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw7x3eqgasj31g50u0wlt.jpg" alt="image-20211108182712231" style="zoom:25%;" />

<p>红色是给一个句子判断种类</p>
<p>绿色是判断两个句子是否是同一个意思</p>
<p>蓝色是NLI任务</p>
<p>还有Chinese GLUE</p>
<p>还有Super GLUE，更难的数据集</p>
<h3 id="DecaNLP"><a href="#DecaNLP" class="headerlink" title="DecaNLP"></a>DecaNLP</h3><p>有十项任务，期待你用一个模型全部解决。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅NLP8:GPT-3</title>
    <url>/2021/11/27/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP8:GPT-3/</url>
    <content><![CDATA[<h1 id="GPT-3"><a href="#GPT-3" class="headerlink" title="GPT-3"></a>GPT-3</h1><img src="https://tva1.sinaimg.cn/large/008i3skNly1gwtnlbgyefj312o0t1tdd.jpg" alt="image-20211127134053874" style="zoom:33%;" />

<p>GPT-3的愿望就是为了：消灭掉Fine-tune步骤，一个Language Model解决所有的问题。这其实是人类的能力，比如一个人去参加英文四六级考试，对于不同的题型，都可以做出回答。</p>
<p>所以GPT-3的Model包括一个Task Description，用来说明要做的任务，然后就可以解题了，如下图所示：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwtno37103j310k0ortbp.jpg" alt="image-20211127134333629" style="zoom: 50%;" />

<p>但是这里的few-shot、one-shot、zero-shot和其原意不同，并不是在训练集里的。而是一种称为“In-context” 的Learning，这些信息都直接作为GPT-3的输入。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwtof3dmwtj312r0qi0y0.jpg" alt="image-20211127140929237" style="zoom: 50%;" />

<p>效果还不错。</p>
<p>但是GPT-3在预训练的时候犯了一个大问题，他忘记把各种实验数据集从预训练语料中删除了，但是影响不大。</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Bert</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅NLP9:Multilingual BERT</title>
    <url>/2021/11/27/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP9:Mutilingual%20BERT/</url>
    <content><![CDATA[<h1 id="Multilingual-BERT"><a href="#Multilingual-BERT" class="headerlink" title="Multilingual BERT"></a>Multilingual BERT</h1>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Bert</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅NLP7:Bert</title>
    <url>/2021/10/27/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP/%E6%9D%8E%E5%AE%8F%E6%AF%85NLP7:Bert/</url>
    <content><![CDATA[<h1 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h1><h2 id="Chapter-One"><a href="#Chapter-One" class="headerlink" title="Chapter One"></a>Chapter One</h2><p>Bert的思路：首先使用大量text生成（Pre-train）一种具有“读懂”人类语言的model，然后针对不同的NLP任务，使用微调Fine-tune。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtx7fhhbej31440u0dji.jpg" alt="image-20211027155313967" style="zoom:33%;" />

<p>科学家们将语言模型的名字命名成芝麻街里的人物（真的是生搬硬凑…）。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtxc85sonj30rs0mcteb.jpg" alt="img" style="zoom: 50%;" />

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtxeuzpzpj313j0u0q97.jpg" alt="image-20211027160022520" style="zoom:33%;" />

<h3 id="What-is-pre-train-model？"><a href="#What-is-pre-train-model？" class="headerlink" title="What is pre-train model？"></a>What is pre-train model？</h3><p>其实在Bert之前就有很多Pre-train模型，比如Word2Vector、GloVe等。</p>
<p>对于Word2Vector Pre-train模型，每输入一个Word都会返回一个向量，缺点就是对于处在不同上下文的同一个词，输出的结果WordEmbedding vector 是一样的，这样并不好。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvtxq906mnj30s60qcdgx.jpg" alt="image-20211027161119424" style="zoom:25%;" />

<p>英语中的Word是非常多的，想把所有的word都有对应的vector几乎是不可能的，而FastText模型可以读入字母来判断word含义。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvty5b29u0j30o60jyq3t.jpg" alt="image-20211027162547845" style="zoom:25%;" />

<p>鉴于中文的象形字，可以使用CNN进行识别。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu29yu3rej30t20qs75v.jpg" alt="image-20211027184834802" style="zoom: 25%;" />

<p>Contextualized word embedding（上下文的，即可以分别同一个word的不同意思），比如Bert，它是首先观察整个上下文，再返回word 的vector。</p>
<p>而bert等模型一直在变大，只能大公司使用和训练，我们“穷人”要做的就是让它们变小。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu2ncv7vkj31540u0n1e.jpg" alt="image-20211027190130144" style="zoom:25%;" />





<h3 id="How-to-pre-train"><a href="#How-to-pre-train" class="headerlink" title="How to pre-train?"></a>How to pre-train?</h3><p>架构为：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu2v3gr43j31610u0jtx.jpg" alt="image-20211027190856616" style="zoom:33%;" />

<h4 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h4><p>分为两种：单句子和多句子，多句子（比如QA问题）需要在句子之间添加[SEP]分隔符。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu2y45o7jj31210u0q57.jpg" alt="image-20211027191150714" style="zoom:33%;" />

<h4 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h4><p>输出分为四种：</p>
<ol>
<li><p>One class:</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu361wl97j30u00xbwgg.jpg" alt="image-20211027191928417" style="zoom:25%;" />
有两种输出方式，一种是是直接输出整个句子的结果，另一种是分别输出句子中单词的word vector然后再加一层网络输出结果。</li>
<li><p>Class for each token：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu3i7rocbj30u00zmgnf.jpg" alt="image-20211027193109874" style="zoom:25%;" />
给每一个token一个class就可以了，可以用LSTM等网络</li>
<li><p>Copy from input：<br>比如extraction-based QA: Document &amp; Query</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu3kpu5q9j31qq0smgrp.jpg" alt="image-20211027193333019" style="zoom:25%;" />
输出是两个整数，即原文的两个单词位置，输出他们之间的句子。
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu3q5wa8wj318l0u00vy.jpg" alt="image-20211027193848141" style="zoom:25%;" /></li>
<li><p>General sequence：<br>可以把Bert当做Encoder，在加一个具体任务的Decoder来产生Seq2Seq模型。<br>但是这样的话，Decoder相当于没有pre-train过，所以要让Decoder尽量小一点，对结果的影响小一点。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu3z86lbaj31o00tead7.jpg" alt="image-20211027194731158" style="zoom:25%;" />
除此之外还有一种做法：

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu9p8wdaej313m0u00vf.jpg" alt="image-20211027230530545" style="zoom:25%;" />

<p>待输入到[SEP]以后，再开始输出一些列的sequence。这样相当于把pre-trained的Model当做了decoder来使用。</p>
</li>
</ol>
<h3 id="How-to-fine-tune"><a href="#How-to-fine-tune" class="headerlink" title="How to fine-tune?"></a>How to fine-tune?</h3><p>Fine-tune有两种基本的方式：一种只微调task-specific，不动bert，另外一种将bert与task-specific合在一起微调。一般后者效果更好一点。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvu9zdo4rlj312t0u0acv.jpg" alt="image-20211027231515279" style="zoom:33%;" />

<h4 id="Adapter"><a href="#Adapter" class="headerlink" title="Adapter"></a>Adapter</h4><p>虽然后者效果好一些，但是涉及到的参数太多，不好调整。这时我们可以将模型里的一层拿出来，只调整这一层，叫Adapter，同时也方便了模型的保存，大部分不变，每个模型只保存Adapter层的参数就好。</p>
<p>Pre-trained的时候没有Adapter，fine-tune的时候才有。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvua67xr1sj319a0u0q6k.jpg" alt="image-20211027232149745" style="zoom: 33%;" />

<h4 id="Weighted-Layer"><a href="#Weighted-Layer" class="headerlink" title="Weighted Layer"></a>Weighted Layer</h4><p>对于Bert等模型的输出的最后两层，可能提取了不同的特征，我们可以用$w_1x_1+w_2x_2$的形式进行加权求和，其中$w_1, w_2$可以当做Task Specific的参数进行学习。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvualrzs5mj313f0u00v0.jpg" alt="image-20211027233647170" style="zoom:33%;" />

<h2 id="Chapter-Two"><a href="#Chapter-Two" class="headerlink" title="Chapter Two"></a>Chapter Two</h2><h3 id="Self-supervised-Learning"><a href="#Self-supervised-Learning" class="headerlink" title="Self-supervised Learning"></a>Self-supervised Learning</h3><img src="https://tva1.sinaimg.cn/large/008i3skNly1gvur68752fj31g20l6409.jpg" alt="image-20211028091002931" style="zoom:33%;" />

<p>自监督学习是一种不需要label的训练方式，以前叫UNsupervised，现在改名字了。</p>
<h3 id="How"><a href="#How" class="headerlink" title="How?"></a>How?</h3><p>第一种方法是以预测下一个token的方式进行自监督学习。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvur9iafmij312p0u0di5.jpg" alt="image-20211028091312448" style="zoom:33%;" />

<p>一个使用LSTM的自监督学习LM是ELMo：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvurc0onvnj31600u077k.jpg" alt="image-20211028091537312" style="zoom:33%;" />

<p>另一些使用self-attendtion的自监督学习模型，如GPT等。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvurejkrosj31600u0tc1.jpg" alt="image-20211028091802989" style="zoom:33%;" />

<p>这种self-attention需要注意让该模型不可以偷看到后面的答案，因为self-Attention模型是首先输入整个seq，然后根据注意力一个个输出token的。如上图所示，$w_2$只可以看$w_1, w_2$，不可以看$w_3$，否则模型就会学会直接将下一个词的向量拿来输出，这样毫无意义。</p>
<h3 id="Why？"><a href="#Why？" class="headerlink" title="Why？"></a>Why？</h3><p>为什么预测下一个词就可以让神经网络学会词的含义呢？因为那句经典的，词的意思由它周围的词决定。</p>
<p>下图中$w_4$只由它前面的词决定，那它右边的词对他的影响呢？</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvurrdestlj311m0u0mzp.jpg" alt="image-20211028093021037" style="zoom:33%;" />

<h3 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h3><p>ELMo模型就考虑了左右两个方向的上下文，对于一个词$w_4$，由一个左向的vector和一个右向的vector共同描述。但这样也有一个问题：左向量在生成的时候不会考虑右向量。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvuru6ej8oj30z40u040p.jpg" alt="image-20211028093303747" style="zoom:33%;" />

<h3 id="Bert-1"><a href="#Bert-1" class="headerlink" title="Bert"></a>Bert</h3><p>Bert就可以兼顾左右两边的vector，它采用扣词的办法，把一个句子里的某个词用[MASK]标志隐藏掉，或者使用随机的token替换掉。这样的模型的Attendtion整个句子的。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvuso4jyanj313n0u0goc.jpg" alt="image-20211028100148962" style="zoom:33%;" />

<h4 id="Masking-Input"><a href="#Masking-Input" class="headerlink" title="Masking Input"></a>Masking Input</h4><p>Whole Word Masking( WWM)：顾名思义，就是每次盖住一个whole word</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvutxqww1rj31s80lajwt.jpg" alt="image-20211028104541957" style="zoom: 33%;" />

<p>Phrase-level &amp; Entity-level: entity就是NER中的entity，需要一个named entity recognition Model配合完成，比较著名的有ERNIE。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvutzv2sa7j31bi0buwfu.jpg" alt="image-20211028104744062" style="zoom:25%;" />

<h4 id="SpanBert"><a href="#SpanBert" class="headerlink" title="SpanBert"></a>SpanBert</h4><p>SpanBert是根据不同概率遮住不同长度的word，效果还算可以：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvuul03tkxj316w0u0adm.jpg" alt="image-20211028110803353" style="zoom:33%;" />

<p>有一种SBO（Span Boundary Objective）的方法来训练SpanBert，试图用Span两边的vector和一个数字（表示要预测的词的位置）来预测该词，好像很匪夷所思，但是在conference里有很大应用：</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvuv1w99q9j31410u076i.jpg" alt="image-20211028112416986" style="zoom:33%;" />

<h4 id="Bert不善言辞？"><a href="#Bert不善言辞？" class="headerlink" title="Bert不善言辞？"></a>Bert不善言辞？</h4><p>Bert模型被设计用来填空，即[MASK]，所以可以认为他不善言辞，不擅长generation，即无法生成sequence语句，做seq2seq的pre-trained Model效果不是很好。当然，这仅限于autoregressive model（即从左到右书写的模型），如果是两边向中间生成的，Bert说不定就很擅长了。 </p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvuvcouu77j31km0u00vu.jpg" alt="image-20211028113439637" style="zoom:33%;" />

<h4 id="MASS-BART"><a href="#MASS-BART" class="headerlink" title="MASS/BART"></a>MASS/BART</h4><p>对于Seq2Seq的，我们要输入一句话然后让Bert可以输出该句话，为了防止直接输出，我们要破坏输入的句子。</p>
<p>以下是几种破坏的方法：</p>
<p>MASS: MAsked Sequence to Sequence pre-training</p>
<p>BART: Bidirectional and Auto-Regressive Transformer</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvuvsnlvi3j314d0u0acu.jpg" alt="image-20211028114959804" style="zoom:33%;" />

<p>MASS就是随机盖住一个词，BART有很多方法，其中Text Infilling效果最好。</p>
<p>Text Infilling就是可以在没有词的地方假装盖住，让模型发现这里没有词，或者一次盖住很多个词，让模型全部补全，或者两者混用。</p>
<p>而Permutation和Rotation效果都很不好，因为他们破坏了模型对句子的认知，模型见到的全是乱七八糟的句子，很难学会什么东西。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvuvw07k8uj31pa0momzj.jpg" alt="image-20211028115313417" style="zoom:25%;" />

<h4 id="UniLM"><a href="#UniLM" class="headerlink" title="UniLM"></a>UniLM</h4><p>一个神奇的Model，它即可作为Encoder也可作为Decoder，也可以作为Encoder+Decoder（即Seq2Seq）。</p>
<p>他可以作为Bert或者GPT（此时只能Attendtion左边的token）或者BART/MASS。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvv0tn7m9nj315e0u0dld.jpg" alt="image-20211028144355962" style="zoom:33%;" />

<h4 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h4><p>ELECTRA用一个small bert输出一个句子，该句子包含一个由small bert篡改的token的句子，然后ELECTRA去句子里抓住这个token。small bert水平不要太好，不然生成的token和原token一样就没法训练ELECTRA了。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvv15ix4dkj30u00yttap.jpg" alt="image-20211028145521973" style="zoom:33%;" />

<p>效果出奇的好，只需要四分之一的数据就能达到和其他主流模型一样的分数。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gvv16uh0i9j31sr0u0tcd.jpg" alt="image-20211028145638008" style="zoom:33%;" />

<h4 id="Sentence-Level-Embedding-Vector"><a href="#Sentence-Level-Embedding-Vector" class="headerlink" title="Sentence-Level Embedding Vector"></a>Sentence-Level Embedding Vector</h4><img src="https://tva1.sinaimg.cn/large/008i3skNly1gw4cg9ts79j314c0u00vo.jpg" alt="image-20211105161616129" style="zoom:33%;" />

<p>句子级别的嵌入向量，与word embedding类似，是sentence embedding。</p>
<p><strong>Skip Thought</strong>：输入这个句子，预测下一个句子。运算速度很慢，对性能要求很高。</p>
<p><strong>Quick Thought：</strong>对每一个句子都是走一遍Encoder，然后用返回的Sentence embedding计算similarity。</p>
<p>在原始的Bert训练中，有两种任务：</p>
<ol>
<li>NSP: Next Sentence Prediction. 判断[SEP]后面的是不是本句子的下一个句子，效果较差。用在RoBERTa中。<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw4d0ualzhj31280m0myl.jpg" alt="image-20211105163604978" style="zoom:33%;" /></li>
<li>SOP: Sentence Order Prediction. 判断这两个句子的顺序是否颠倒。用在AlBERT和structBERT(Alice)中。</li>
</ol>
<h3 id="ERNIE"><a href="#ERNIE" class="headerlink" title="ERNIE"></a>ERNIE</h3><p>BERT加上external knowledge就进化成了ERNIE。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gw4d6rrxh1j31k70u0afj.jpg" alt="image-20211105164145380" style="zoom:25%;" />









 



















]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Bert</tag>
      </tags>
  </entry>
</search>
